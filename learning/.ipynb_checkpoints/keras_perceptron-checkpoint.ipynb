{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 汎用ライブラリのimport\n",
    "import sys\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from itertools import chain\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras用ライブラリ\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自作ライブラリのimport\n",
    "if os.environ['BR_HOME']+\"/boatrace\" not in sys.path:\n",
    "    sys.path.append(os.environ['BR_HOME']+\"/boatrace\")\n",
    "#print(sys.path)\n",
    "\n",
    "from setup.myUtil import dbHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 舟券の配列を取得\n",
    "funakenList=[]\n",
    "with open(os.environ['BR_HOME']+'/boatrace/config/3t_list.dat') as f:\n",
    "    reader=csv.reader(f)\n",
    "    for row in reader:\n",
    "        funakenList.append(row)\n",
    "funakenID = [i for i in range(120)]\n",
    "funakenDict=dict(zip(funakenList[0],funakenID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析期間の指定は一旦ここでまとめてみる。\n",
    "simStartDate=\"20190101\"\n",
    "simEndDate=\"20190531\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbh=dbHandler.getDBHandle()\n",
    "#dbHandler.closeDBHandle(dbh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simdata: 22904\n"
     ]
    }
   ],
   "source": [
    "# データを取得\n",
    "with dbh.cursor() as cursor:\n",
    "    sel_sql = \"select * from raceabst_forml_rentai_v \\\n",
    "               where raceDate between '%s' and '%s' \\\n",
    "               order by raceId \"\\\n",
    "               % (simStartDate,simEndDate)\n",
    "    cursor.execute(sel_sql)\n",
    "    loadList=cursor.fetchall()\n",
    "print(\"simdata:\",len(loadList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>funaken</th>\n",
       "      <th>l1Fcnt</th>\n",
       "      <th>l1boat2r</th>\n",
       "      <th>l1boat3r</th>\n",
       "      <th>l1motor2r</th>\n",
       "      <th>l1motor3r</th>\n",
       "      <th>l1oldavgstdev</th>\n",
       "      <th>l1oldavgsttime</th>\n",
       "      <th>l1oldrank1</th>\n",
       "      <th>l1oldrank2</th>\n",
       "      <th>...</th>\n",
       "      <th>l6oldrank4</th>\n",
       "      <th>l6oldrank5</th>\n",
       "      <th>l6oldrank6</th>\n",
       "      <th>l6rank</th>\n",
       "      <th>l6score</th>\n",
       "      <th>odds</th>\n",
       "      <th>raceDate</th>\n",
       "      <th>raceId</th>\n",
       "      <th>raceWaveHeight</th>\n",
       "      <th>raceWindSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-2-5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4074</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.3061</td>\n",
       "      <td>0.4558</td>\n",
       "      <td>0.00115</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>...</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>B1</td>\n",
       "      <td>1.464794</td>\n",
       "      <td>14.8</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>20190101-06-01</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3-4-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4717</td>\n",
       "      <td>0.6038</td>\n",
       "      <td>0.2687</td>\n",
       "      <td>0.4104</td>\n",
       "      <td>-0.02393</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>B1</td>\n",
       "      <td>0.749802</td>\n",
       "      <td>63.8</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>20190101-06-02</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6-4-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.4875</td>\n",
       "      <td>0.6375</td>\n",
       "      <td>0.01078</td>\n",
       "      <td>0.1824</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>A1</td>\n",
       "      <td>8.404421</td>\n",
       "      <td>29.9</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>20190101-06-03</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-3-6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5370</td>\n",
       "      <td>0.3916</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>0.00080</td>\n",
       "      <td>0.1676</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>B1</td>\n",
       "      <td>1.101158</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>20190101-06-04</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6-2-3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4082</td>\n",
       "      <td>0.5102</td>\n",
       "      <td>0.2568</td>\n",
       "      <td>0.4865</td>\n",
       "      <td>0.02388</td>\n",
       "      <td>0.2003</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>A1</td>\n",
       "      <td>3.301202</td>\n",
       "      <td>16.3</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>20190101-06-05</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  funaken  l1Fcnt  l1boat2r  l1boat3r  l1motor2r  l1motor3r  l1oldavgstdev  \\\n",
       "0   1-2-5       0    0.4074    0.5556     0.3061     0.4558        0.00115   \n",
       "1   3-4-1       0    0.4717    0.6038     0.2687     0.4104       -0.02393   \n",
       "2   6-4-1       0    0.3704    0.5556     0.4875     0.6375        0.01078   \n",
       "3   1-3-6       0    0.3704    0.5370     0.3916     0.5060        0.00080   \n",
       "4   6-2-3       0    0.4082    0.5102     0.2568     0.4865        0.02388   \n",
       "\n",
       "   l1oldavgsttime  l1oldrank1  l1oldrank2  ...  l6oldrank4  l6oldrank5  \\\n",
       "0          0.1810        0.04        0.14  ...        0.22        0.23   \n",
       "1          0.1625        0.18        0.15  ...        0.21        0.14   \n",
       "2          0.1824        0.10        0.12  ...        0.12        0.16   \n",
       "3          0.1676        0.33        0.17  ...        0.26        0.14   \n",
       "4          0.2003        0.00        0.04  ...        0.12        0.10   \n",
       "\n",
       "   l6oldrank6  l6rank   l6score  odds    raceDate          raceId  \\\n",
       "0        0.18      B1  1.464794  14.8  2019-01-01  20190101-06-01   \n",
       "1        0.19      B1  0.749802  63.8  2019-01-01  20190101-06-02   \n",
       "2        0.16      A1  8.404421  29.9  2019-01-01  20190101-06-03   \n",
       "3        0.17      B1  1.101158   5.3  2019-01-01  20190101-06-04   \n",
       "4        0.09      A1  3.301202  16.3  2019-01-01  20190101-06-05   \n",
       "\n",
       "   raceWaveHeight  raceWindSpeed  \n",
       "0               2            3.0  \n",
       "1               2            3.0  \n",
       "2               2            3.0  \n",
       "3               2            3.0  \n",
       "4               2            3.0  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.io.json.json_normalize(loadList)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1Fcnt</th>\n",
       "      <th>l1boat2r</th>\n",
       "      <th>l1boat3r</th>\n",
       "      <th>l1motor2r</th>\n",
       "      <th>l1motor3r</th>\n",
       "      <th>l1oldavgstdev</th>\n",
       "      <th>l1oldavgsttime</th>\n",
       "      <th>l1oldrank1</th>\n",
       "      <th>l1oldrank2</th>\n",
       "      <th>l1oldrank3</th>\n",
       "      <th>...</th>\n",
       "      <th>l6oldavgsttime</th>\n",
       "      <th>l6oldrank1</th>\n",
       "      <th>l6oldrank2</th>\n",
       "      <th>l6oldrank3</th>\n",
       "      <th>l6oldrank4</th>\n",
       "      <th>l6oldrank5</th>\n",
       "      <th>l6oldrank6</th>\n",
       "      <th>l6rank</th>\n",
       "      <th>raceWaveHeight</th>\n",
       "      <th>raceWindSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.4074</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.3061</td>\n",
       "      <td>0.4558</td>\n",
       "      <td>0.00115</td>\n",
       "      <td>0.1810</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1867</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.18</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.4717</td>\n",
       "      <td>0.6038</td>\n",
       "      <td>0.2687</td>\n",
       "      <td>0.4104</td>\n",
       "      <td>-0.02393</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1730</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5556</td>\n",
       "      <td>0.4875</td>\n",
       "      <td>0.6375</td>\n",
       "      <td>0.01078</td>\n",
       "      <td>0.1824</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1254</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.3704</td>\n",
       "      <td>0.5370</td>\n",
       "      <td>0.3916</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>0.00080</td>\n",
       "      <td>0.1676</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>...</td>\n",
       "      <td>0.2045</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.17</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.4082</td>\n",
       "      <td>0.5102</td>\n",
       "      <td>0.2568</td>\n",
       "      <td>0.4865</td>\n",
       "      <td>0.02388</td>\n",
       "      <td>0.2003</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1540</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   l1Fcnt  l1boat2r  l1boat3r  l1motor2r  l1motor3r  l1oldavgstdev  \\\n",
       "0       0    0.4074    0.5556     0.3061     0.4558        0.00115   \n",
       "1       0    0.4717    0.6038     0.2687     0.4104       -0.02393   \n",
       "2       0    0.3704    0.5556     0.4875     0.6375        0.01078   \n",
       "3       0    0.3704    0.5370     0.3916     0.5060        0.00080   \n",
       "4       0    0.4082    0.5102     0.2568     0.4865        0.02388   \n",
       "\n",
       "   l1oldavgsttime  l1oldrank1  l1oldrank2  l1oldrank3  ...  l6oldavgsttime  \\\n",
       "0          0.1810        0.04        0.14        0.31  ...          0.1867   \n",
       "1          0.1625        0.18        0.15        0.13  ...          0.1730   \n",
       "2          0.1824        0.10        0.12        0.13  ...          0.1254   \n",
       "3          0.1676        0.33        0.17        0.20  ...          0.2045   \n",
       "4          0.2003        0.00        0.04        0.04  ...          0.1540   \n",
       "\n",
       "   l6oldrank1  l6oldrank2  l6oldrank3  l6oldrank4  l6oldrank5  l6oldrank6  \\\n",
       "0        0.09        0.15        0.13        0.22        0.23        0.18   \n",
       "1        0.18        0.12        0.16        0.21        0.14        0.19   \n",
       "2        0.22        0.18        0.16        0.12        0.16        0.16   \n",
       "3        0.08        0.18        0.17        0.26        0.14        0.17   \n",
       "4        0.31        0.17        0.21        0.12        0.10        0.09   \n",
       "\n",
       "   l6rank  raceWaveHeight  raceWindSpeed  \n",
       "0       2               2            3.0  \n",
       "1       2               2            3.0  \n",
       "2       0               2            3.0  \n",
       "3       2               2            3.0  \n",
       "4       0               2            3.0  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 入力のデータ整形\n",
    "xdf=df.drop(['funaken','odds','raceId','raceDate'],axis=1)\n",
    "# オッズから作ったスコアは効きすぎるので捨ててみる\n",
    "xdf=xdf.drop(['l1score','l2score','l3score','l4score','l5score','l6score'],axis=1)\n",
    "#xdf=xdf.drop(['l1Fcnt','l2Fcnt','l3Fcnt','l4Fcnt','l5Fcnt','l6Fcnt'],axis=1)\n",
    "#xdf=xdf.drop(['l1oldavgstdev','l2oldavgstdev','l3oldavgstdev','l4oldavgstdev','l5oldavgstdev','l6oldavgstdev'],axis=1)\n",
    "\n",
    "rankLabel=LabelEncoder()\n",
    "rankLabel=rankLabel.fit(xdf['l1rank'])\n",
    "xdf['l1rank']=rankLabel.transform(xdf['l1rank'])\n",
    "xdf['l2rank']=rankLabel.transform(xdf['l2rank'])\n",
    "xdf['l3rank']=rankLabel.transform(xdf['l3rank'])\n",
    "xdf['l4rank']=rankLabel.transform(xdf['l4rank'])\n",
    "xdf['l5rank']=rankLabel.transform(xdf['l5rank'])\n",
    "xdf['l6rank']=rankLabel.transform(xdf['l6rank'])\n",
    "xdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizeしておく\n",
    "# ★validationの時に要注意。\n",
    "xdf=xdf.apply(scipy.stats.zscore, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>l1Fcnt</th>\n",
       "      <th>l1boat2r</th>\n",
       "      <th>l1boat3r</th>\n",
       "      <th>l1motor2r</th>\n",
       "      <th>l1motor3r</th>\n",
       "      <th>l1oldavgstdev</th>\n",
       "      <th>l1oldavgsttime</th>\n",
       "      <th>l1oldrank1</th>\n",
       "      <th>l1oldrank2</th>\n",
       "      <th>l1oldrank3</th>\n",
       "      <th>...</th>\n",
       "      <th>l6oldavgsttime</th>\n",
       "      <th>l6oldrank1</th>\n",
       "      <th>l6oldrank2</th>\n",
       "      <th>l6oldrank3</th>\n",
       "      <th>l6oldrank4</th>\n",
       "      <th>l6oldrank5</th>\n",
       "      <th>l6oldrank6</th>\n",
       "      <th>l6rank</th>\n",
       "      <th>raceWaveHeight</th>\n",
       "      <th>raceWindSpeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.404206</td>\n",
       "      <td>0.802886</td>\n",
       "      <td>0.539349</td>\n",
       "      <td>-0.160546</td>\n",
       "      <td>-0.194103</td>\n",
       "      <td>0.093684</td>\n",
       "      <td>-0.050510</td>\n",
       "      <td>-1.539874</td>\n",
       "      <td>-0.735871</td>\n",
       "      <td>2.929335</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018185</td>\n",
       "      <td>-0.587633</td>\n",
       "      <td>-0.042491</td>\n",
       "      <td>-0.400240</td>\n",
       "      <td>1.053744</td>\n",
       "      <td>0.993327</td>\n",
       "      <td>-0.078889</td>\n",
       "      <td>0.559703</td>\n",
       "      <td>-0.292066</td>\n",
       "      <td>0.039602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.404206</td>\n",
       "      <td>1.415346</td>\n",
       "      <td>0.899224</td>\n",
       "      <td>-0.476856</td>\n",
       "      <td>-0.497122</td>\n",
       "      <td>-0.506313</td>\n",
       "      <td>-0.430154</td>\n",
       "      <td>-0.117148</td>\n",
       "      <td>-0.553387</td>\n",
       "      <td>-0.928423</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.299386</td>\n",
       "      <td>0.283559</td>\n",
       "      <td>-0.451575</td>\n",
       "      <td>0.068753</td>\n",
       "      <td>0.882839</td>\n",
       "      <td>-0.391612</td>\n",
       "      <td>-0.012798</td>\n",
       "      <td>0.559703</td>\n",
       "      <td>-0.292066</td>\n",
       "      <td>0.039602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.404206</td>\n",
       "      <td>0.450459</td>\n",
       "      <td>0.539349</td>\n",
       "      <td>1.373641</td>\n",
       "      <td>1.018642</td>\n",
       "      <td>0.324065</td>\n",
       "      <td>-0.021780</td>\n",
       "      <td>-0.930134</td>\n",
       "      <td>-1.100839</td>\n",
       "      <td>-0.928423</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.276403</td>\n",
       "      <td>0.670756</td>\n",
       "      <td>0.366593</td>\n",
       "      <td>0.068753</td>\n",
       "      <td>-0.655300</td>\n",
       "      <td>-0.083848</td>\n",
       "      <td>-0.211073</td>\n",
       "      <td>-1.491899</td>\n",
       "      <td>-0.292066</td>\n",
       "      <td>0.039602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.404206</td>\n",
       "      <td>0.450459</td>\n",
       "      <td>0.400476</td>\n",
       "      <td>0.562569</td>\n",
       "      <td>0.140954</td>\n",
       "      <td>0.085310</td>\n",
       "      <td>-0.325495</td>\n",
       "      <td>1.407201</td>\n",
       "      <td>-0.188419</td>\n",
       "      <td>0.571816</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347170</td>\n",
       "      <td>-0.684432</td>\n",
       "      <td>0.366593</td>\n",
       "      <td>0.225084</td>\n",
       "      <td>1.737362</td>\n",
       "      <td>-0.391612</td>\n",
       "      <td>-0.144981</td>\n",
       "      <td>0.559703</td>\n",
       "      <td>-0.292066</td>\n",
       "      <td>0.039602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.404206</td>\n",
       "      <td>0.810506</td>\n",
       "      <td>0.200380</td>\n",
       "      <td>-0.577500</td>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.637461</td>\n",
       "      <td>0.345551</td>\n",
       "      <td>-1.946367</td>\n",
       "      <td>-2.560709</td>\n",
       "      <td>-2.857302</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.689372</td>\n",
       "      <td>1.541948</td>\n",
       "      <td>0.230232</td>\n",
       "      <td>0.850409</td>\n",
       "      <td>-0.655300</td>\n",
       "      <td>-1.007140</td>\n",
       "      <td>-0.673715</td>\n",
       "      <td>-1.491899</td>\n",
       "      <td>-0.292066</td>\n",
       "      <td>0.039602</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 86 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     l1Fcnt  l1boat2r  l1boat3r  l1motor2r  l1motor3r  l1oldavgstdev  \\\n",
       "0 -0.404206  0.802886  0.539349  -0.160546  -0.194103       0.093684   \n",
       "1 -0.404206  1.415346  0.899224  -0.476856  -0.497122      -0.506313   \n",
       "2 -0.404206  0.450459  0.539349   1.373641   1.018642       0.324065   \n",
       "3 -0.404206  0.450459  0.400476   0.562569   0.140954       0.085310   \n",
       "4 -0.404206  0.810506  0.200380  -0.577500   0.010802       0.637461   \n",
       "\n",
       "   l1oldavgsttime  l1oldrank1  l1oldrank2  l1oldrank3  ...  l6oldavgsttime  \\\n",
       "0       -0.050510   -1.539874   -0.735871    2.929335  ...       -0.018185   \n",
       "1       -0.430154   -0.117148   -0.553387   -0.928423  ...       -0.299386   \n",
       "2       -0.021780   -0.930134   -1.100839   -0.928423  ...       -1.276403   \n",
       "3       -0.325495    1.407201   -0.188419    0.571816  ...        0.347170   \n",
       "4        0.345551   -1.946367   -2.560709   -2.857302  ...       -0.689372   \n",
       "\n",
       "   l6oldrank1  l6oldrank2  l6oldrank3  l6oldrank4  l6oldrank5  l6oldrank6  \\\n",
       "0   -0.587633   -0.042491   -0.400240    1.053744    0.993327   -0.078889   \n",
       "1    0.283559   -0.451575    0.068753    0.882839   -0.391612   -0.012798   \n",
       "2    0.670756    0.366593    0.068753   -0.655300   -0.083848   -0.211073   \n",
       "3   -0.684432    0.366593    0.225084    1.737362   -0.391612   -0.144981   \n",
       "4    1.541948    0.230232    0.850409   -0.655300   -1.007140   -0.673715   \n",
       "\n",
       "     l6rank  raceWaveHeight  raceWindSpeed  \n",
       "0  0.559703       -0.292066       0.039602  \n",
       "1  0.559703       -0.292066       0.039602  \n",
       "2 -1.491899       -0.292066       0.039602  \n",
       "3  0.559703       -0.292066       0.039602  \n",
       "4 -1.491899       -0.292066       0.039602  \n",
       "\n",
       "[5 rows x 86 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# ファイルから作った辞書で変換する\n",
    "ydf=df['funaken']\n",
    "ydf=pd.DataFrame(ydf.replace(funakenDict))\n",
    "ydf['funaken']=ydf['funaken'].astype(int)\n",
    "#ydf.head()\n",
    "\n",
    "# ydfはone-hotにしておく。\n",
    "y=np.eye(120)[ydf['funaken']]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14.8 63.8 29.9 ...  6.  15.7  5.3]\n",
      "['20190101-06-01' '20190101-06-02' '20190101-06-03' ... '20190531-23-10'\n",
      " '20190531-23-11' '20190531-23-12']\n"
     ]
    }
   ],
   "source": [
    "# 重み付けのため、オッズのリストを作る\n",
    "odf=df['odds'].values\n",
    "raceId_df=df['raceId'].values\n",
    "#odf=pd.DataFrame(df['odds'])\n",
    "#odf.describe()\n",
    "print(odf)\n",
    "print(raceId_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train,X_test: 17178 5726\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "y_train,y_test: 17178 5726\n",
      "<class 'numpy.ndarray'>\n",
      "o_train,o_test: 17178 5726\n",
      "<class 'numpy.ndarray'>\n",
      "raceId_train,raceId_test: 17178 5726\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#train/testを分割\n",
    "X_train, X_test, y_train, y_test,o_train,o_test,raceId_train,raceId_test = train_test_split(xdf, y,odf,raceId_df)\n",
    "print(\"X_train,X_test:\",len(X_train),len(X_test))\n",
    "print(type(X_train))\n",
    "print(\"y_train,y_test:\",len(y_train),len(y_test))\n",
    "print(type(y_train))\n",
    "print(\"o_train,o_test:\",len(o_train),len(o_test))\n",
    "print(type(o_train))\n",
    "print(\"raceId_train,raceId_test:\",len(raceId_train),len(raceId_test))\n",
    "print(type(o_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class\n",
    "class my_Net:\n",
    "    def __init__(self):\n",
    "        self.Epoch = 200\n",
    "        self.Batch_size = 8000\n",
    "        self.Verbose = 1 #ログの出力モード切替 : 1 プログレスバーで表示\n",
    "        self.output_size = 120\n",
    "        self.optimize = SGD()\n",
    "        #self.hidden_units = 128\n",
    "        self.hidden_units = 256\n",
    "        self.Validation_split = 0.2 #訓練データの中で検証データとして扱う割合\n",
    "        #self.Reshape = 28 * 28\n",
    "        \n",
    "        self.model = Sequential()\n",
    " \n",
    "    def make_net(self):\n",
    "        self.model.add(Dense(self.hidden_units, input_shape=(86,)))\n",
    "        self.model.add(Activation(\"relu\"))\n",
    "        self.model.add(Dense(self.hidden_units))\n",
    "        self.model.add(Activation(\"relu\"))\n",
    "        self.model.add(Dense(self.hidden_units))\n",
    "        self.model.add(Activation(\"relu\"))\n",
    "        self.model.add(Dense(self.hidden_units))\n",
    "        self.model.add(Activation(\"relu\"))\n",
    "        self.model.add(Dense(self.output_size))\n",
    "        self.model.add(Activation(\"softmax\"))\n",
    "        self.model.summary()\n",
    " \n",
    "    def model_compile(self):\n",
    "        self.model.compile(loss=\"categorical_crossentropy\", #交差エントロピー誤差関数\n",
    "                           optimizer=self.optimize,#確率的勾配下降法\n",
    "                           metrics=[\"accuracy\"])\n",
    " \n",
    "    def make_model(self):\n",
    "        self.make_net()\n",
    "        self.model_compile()\n",
    " \n",
    "    def train(self, x, t):\n",
    "        self.model.fit(x, t, batch_size=self.Batch_size,\n",
    "                       epochs=self.Epoch,\n",
    "                       verbose=self.Verbose,\n",
    "                       validation_split=self.Validation_split)\n",
    " \n",
    "    def score(self, x, t):\n",
    "        return self.model.evaluate(x, t, verbose=self.Verbose)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_78 (Dense)             (None, 256)               22272     \n",
      "_________________________________________________________________\n",
      "activation_78 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_79 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_80 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "activation_81 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 120)               30840     \n",
      "_________________________________________________________________\n",
      "activation_82 (Activation)   (None, 120)               0         \n",
      "=================================================================\n",
      "Total params: 250,488\n",
      "Trainable params: 250,488\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 13742 samples, validate on 3436 samples\n",
      "Epoch 1/200\n",
      "13742/13742 [==============================] - 1s 105us/step - loss: 4.7621 - acc: 0.0306 - val_loss: 4.7581 - val_acc: 0.0346\n",
      "Epoch 2/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.7564 - acc: 0.0347 - val_loss: 4.7525 - val_acc: 0.0387\n",
      "Epoch 3/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.7506 - acc: 0.0396 - val_loss: 4.7469 - val_acc: 0.0422\n",
      "Epoch 4/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.7449 - acc: 0.0438 - val_loss: 4.7414 - val_acc: 0.0486\n",
      "Epoch 5/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.7391 - acc: 0.0480 - val_loss: 4.7358 - val_acc: 0.0515\n",
      "Epoch 6/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.7334 - acc: 0.0520 - val_loss: 4.7302 - val_acc: 0.0547\n",
      "Epoch 7/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.7277 - acc: 0.0554 - val_loss: 4.7247 - val_acc: 0.0573\n",
      "Epoch 8/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.7220 - acc: 0.0588 - val_loss: 4.7191 - val_acc: 0.0602\n",
      "Epoch 9/200\n",
      "13742/13742 [==============================] - 0s 27us/step - loss: 4.7163 - acc: 0.0617 - val_loss: 4.7135 - val_acc: 0.0626\n",
      "Epoch 10/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.7105 - acc: 0.0643 - val_loss: 4.7078 - val_acc: 0.0640\n",
      "Epoch 11/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.7047 - acc: 0.0667 - val_loss: 4.7022 - val_acc: 0.0649\n",
      "Epoch 12/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6989 - acc: 0.0680 - val_loss: 4.6965 - val_acc: 0.0664\n",
      "Epoch 13/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6930 - acc: 0.0689 - val_loss: 4.6908 - val_acc: 0.0666\n",
      "Epoch 14/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6871 - acc: 0.0696 - val_loss: 4.6850 - val_acc: 0.0681\n",
      "Epoch 15/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6812 - acc: 0.0703 - val_loss: 4.6792 - val_acc: 0.0681\n",
      "Epoch 16/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6753 - acc: 0.0711 - val_loss: 4.6733 - val_acc: 0.0687\n",
      "Epoch 17/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6693 - acc: 0.0719 - val_loss: 4.6675 - val_acc: 0.0693\n",
      "Epoch 18/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6632 - acc: 0.0726 - val_loss: 4.6615 - val_acc: 0.0698\n",
      "Epoch 19/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6571 - acc: 0.0726 - val_loss: 4.6556 - val_acc: 0.0701\n",
      "Epoch 20/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6510 - acc: 0.0726 - val_loss: 4.6495 - val_acc: 0.0701\n",
      "Epoch 21/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6448 - acc: 0.0724 - val_loss: 4.6435 - val_acc: 0.0701\n",
      "Epoch 22/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6385 - acc: 0.0724 - val_loss: 4.6373 - val_acc: 0.0701\n",
      "Epoch 23/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6322 - acc: 0.0723 - val_loss: 4.6312 - val_acc: 0.0701\n",
      "Epoch 24/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6258 - acc: 0.0724 - val_loss: 4.6250 - val_acc: 0.0701\n",
      "Epoch 25/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6194 - acc: 0.0724 - val_loss: 4.6187 - val_acc: 0.0701\n",
      "Epoch 26/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6129 - acc: 0.0725 - val_loss: 4.6124 - val_acc: 0.0701\n",
      "Epoch 27/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.6064 - acc: 0.0725 - val_loss: 4.6060 - val_acc: 0.0701\n",
      "Epoch 28/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5998 - acc: 0.0725 - val_loss: 4.5996 - val_acc: 0.0701\n",
      "Epoch 29/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5932 - acc: 0.0725 - val_loss: 4.5932 - val_acc: 0.0701\n",
      "Epoch 30/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5865 - acc: 0.0725 - val_loss: 4.5867 - val_acc: 0.0701\n",
      "Epoch 31/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5798 - acc: 0.0725 - val_loss: 4.5802 - val_acc: 0.0701\n",
      "Epoch 32/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5731 - acc: 0.0725 - val_loss: 4.5737 - val_acc: 0.0701\n",
      "Epoch 33/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5664 - acc: 0.0725 - val_loss: 4.5672 - val_acc: 0.0701\n",
      "Epoch 34/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5597 - acc: 0.0725 - val_loss: 4.5607 - val_acc: 0.0701\n",
      "Epoch 35/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5529 - acc: 0.0725 - val_loss: 4.5542 - val_acc: 0.0701\n",
      "Epoch 36/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5461 - acc: 0.0725 - val_loss: 4.5476 - val_acc: 0.0701\n",
      "Epoch 37/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5394 - acc: 0.0725 - val_loss: 4.5411 - val_acc: 0.0701\n",
      "Epoch 38/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5326 - acc: 0.0725 - val_loss: 4.5346 - val_acc: 0.0701\n",
      "Epoch 39/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5259 - acc: 0.0725 - val_loss: 4.5282 - val_acc: 0.0701\n",
      "Epoch 40/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5192 - acc: 0.0725 - val_loss: 4.5218 - val_acc: 0.0701\n",
      "Epoch 41/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5126 - acc: 0.0725 - val_loss: 4.5155 - val_acc: 0.0701\n",
      "Epoch 42/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.5060 - acc: 0.0725 - val_loss: 4.5091 - val_acc: 0.0701\n",
      "Epoch 43/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4994 - acc: 0.0725 - val_loss: 4.5029 - val_acc: 0.0701\n",
      "Epoch 44/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4929 - acc: 0.0725 - val_loss: 4.4967 - val_acc: 0.0701\n",
      "Epoch 45/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4865 - acc: 0.0725 - val_loss: 4.4907 - val_acc: 0.0701\n",
      "Epoch 46/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4802 - acc: 0.0725 - val_loss: 4.4847 - val_acc: 0.0701\n",
      "Epoch 47/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4739 - acc: 0.0725 - val_loss: 4.4788 - val_acc: 0.0701\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4678 - acc: 0.0725 - val_loss: 4.4730 - val_acc: 0.0701\n",
      "Epoch 49/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4618 - acc: 0.0725 - val_loss: 4.4674 - val_acc: 0.0701\n",
      "Epoch 50/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4559 - acc: 0.0725 - val_loss: 4.4618 - val_acc: 0.0701\n",
      "Epoch 51/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4500 - acc: 0.0725 - val_loss: 4.4563 - val_acc: 0.0701\n",
      "Epoch 52/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4443 - acc: 0.0725 - val_loss: 4.4510 - val_acc: 0.0701\n",
      "Epoch 53/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4387 - acc: 0.0725 - val_loss: 4.4458 - val_acc: 0.0701\n",
      "Epoch 54/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4332 - acc: 0.0725 - val_loss: 4.4407 - val_acc: 0.0701\n",
      "Epoch 55/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4279 - acc: 0.0725 - val_loss: 4.4357 - val_acc: 0.0701\n",
      "Epoch 56/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4226 - acc: 0.0725 - val_loss: 4.4308 - val_acc: 0.0701\n",
      "Epoch 57/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4175 - acc: 0.0725 - val_loss: 4.4260 - val_acc: 0.0701\n",
      "Epoch 58/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4124 - acc: 0.0725 - val_loss: 4.4214 - val_acc: 0.0701\n",
      "Epoch 59/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4075 - acc: 0.0725 - val_loss: 4.4168 - val_acc: 0.0701\n",
      "Epoch 60/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.4027 - acc: 0.0725 - val_loss: 4.4124 - val_acc: 0.0701\n",
      "Epoch 61/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3980 - acc: 0.0725 - val_loss: 4.4081 - val_acc: 0.0701\n",
      "Epoch 62/200\n",
      "13742/13742 [==============================] - 0s 27us/step - loss: 4.3935 - acc: 0.0725 - val_loss: 4.4039 - val_acc: 0.0701\n",
      "Epoch 63/200\n",
      "13742/13742 [==============================] - 0s 27us/step - loss: 4.3890 - acc: 0.0725 - val_loss: 4.3998 - val_acc: 0.0701\n",
      "Epoch 64/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3846 - acc: 0.0725 - val_loss: 4.3957 - val_acc: 0.0701\n",
      "Epoch 65/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3804 - acc: 0.0725 - val_loss: 4.3918 - val_acc: 0.0701\n",
      "Epoch 66/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3762 - acc: 0.0725 - val_loss: 4.3880 - val_acc: 0.0701\n",
      "Epoch 67/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3722 - acc: 0.0725 - val_loss: 4.3843 - val_acc: 0.0701\n",
      "Epoch 68/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3682 - acc: 0.0725 - val_loss: 4.3807 - val_acc: 0.0701\n",
      "Epoch 69/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3644 - acc: 0.0725 - val_loss: 4.3772 - val_acc: 0.0701\n",
      "Epoch 70/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3606 - acc: 0.0725 - val_loss: 4.3738 - val_acc: 0.0698\n",
      "Epoch 71/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3569 - acc: 0.0726 - val_loss: 4.3704 - val_acc: 0.0698\n",
      "Epoch 72/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3533 - acc: 0.0725 - val_loss: 4.3672 - val_acc: 0.0696\n",
      "Epoch 73/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3498 - acc: 0.0725 - val_loss: 4.3640 - val_acc: 0.0696\n",
      "Epoch 74/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3464 - acc: 0.0725 - val_loss: 4.3609 - val_acc: 0.0696\n",
      "Epoch 75/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3431 - acc: 0.0726 - val_loss: 4.3579 - val_acc: 0.0696\n",
      "Epoch 76/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3398 - acc: 0.0726 - val_loss: 4.3549 - val_acc: 0.0696\n",
      "Epoch 77/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3367 - acc: 0.0726 - val_loss: 4.3521 - val_acc: 0.0693\n",
      "Epoch 78/200\n",
      "13742/13742 [==============================] - 0s 27us/step - loss: 4.3336 - acc: 0.0726 - val_loss: 4.3493 - val_acc: 0.0693\n",
      "Epoch 79/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3306 - acc: 0.0726 - val_loss: 4.3465 - val_acc: 0.0693\n",
      "Epoch 80/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3276 - acc: 0.0726 - val_loss: 4.3439 - val_acc: 0.0693\n",
      "Epoch 81/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3248 - acc: 0.0726 - val_loss: 4.3413 - val_acc: 0.0687\n",
      "Epoch 82/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3220 - acc: 0.0726 - val_loss: 4.3388 - val_acc: 0.0687\n",
      "Epoch 83/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3192 - acc: 0.0726 - val_loss: 4.3363 - val_acc: 0.0687\n",
      "Epoch 84/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3165 - acc: 0.0726 - val_loss: 4.3339 - val_acc: 0.0690\n",
      "Epoch 85/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3139 - acc: 0.0723 - val_loss: 4.3315 - val_acc: 0.0690\n",
      "Epoch 86/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3113 - acc: 0.0722 - val_loss: 4.3292 - val_acc: 0.0687\n",
      "Epoch 87/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3088 - acc: 0.0722 - val_loss: 4.3270 - val_acc: 0.0684\n",
      "Epoch 88/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3064 - acc: 0.0723 - val_loss: 4.3248 - val_acc: 0.0684\n",
      "Epoch 89/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3040 - acc: 0.0722 - val_loss: 4.3227 - val_acc: 0.0684\n",
      "Epoch 90/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.3016 - acc: 0.0720 - val_loss: 4.3206 - val_acc: 0.0684\n",
      "Epoch 91/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2993 - acc: 0.0721 - val_loss: 4.3185 - val_acc: 0.0684\n",
      "Epoch 92/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2971 - acc: 0.0720 - val_loss: 4.3165 - val_acc: 0.0687\n",
      "Epoch 93/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2949 - acc: 0.0720 - val_loss: 4.3146 - val_acc: 0.0690\n",
      "Epoch 94/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2927 - acc: 0.0722 - val_loss: 4.3127 - val_acc: 0.0690\n",
      "Epoch 95/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2906 - acc: 0.0720 - val_loss: 4.3108 - val_acc: 0.0687\n",
      "Epoch 96/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2886 - acc: 0.0719 - val_loss: 4.3089 - val_acc: 0.0687\n",
      "Epoch 97/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2866 - acc: 0.0720 - val_loss: 4.3072 - val_acc: 0.0687\n",
      "Epoch 98/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2846 - acc: 0.0719 - val_loss: 4.3054 - val_acc: 0.0687\n",
      "Epoch 99/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2827 - acc: 0.0722 - val_loss: 4.3037 - val_acc: 0.0687\n",
      "Epoch 100/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2808 - acc: 0.0722 - val_loss: 4.3020 - val_acc: 0.0687\n",
      "Epoch 101/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2789 - acc: 0.0720 - val_loss: 4.3004 - val_acc: 0.0681\n",
      "Epoch 102/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2771 - acc: 0.0720 - val_loss: 4.2988 - val_acc: 0.0681\n",
      "Epoch 103/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2753 - acc: 0.0718 - val_loss: 4.2973 - val_acc: 0.0681\n",
      "Epoch 104/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2736 - acc: 0.0718 - val_loss: 4.2957 - val_acc: 0.0681\n",
      "Epoch 105/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2719 - acc: 0.0716 - val_loss: 4.2942 - val_acc: 0.0681\n",
      "Epoch 106/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2703 - acc: 0.0716 - val_loss: 4.2928 - val_acc: 0.0690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 107/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2686 - acc: 0.0718 - val_loss: 4.2913 - val_acc: 0.0693\n",
      "Epoch 108/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2670 - acc: 0.0719 - val_loss: 4.2899 - val_acc: 0.0693\n",
      "Epoch 109/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2655 - acc: 0.0718 - val_loss: 4.2886 - val_acc: 0.0693\n",
      "Epoch 110/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2640 - acc: 0.0718 - val_loss: 4.2872 - val_acc: 0.0687\n",
      "Epoch 111/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2625 - acc: 0.0718 - val_loss: 4.2859 - val_acc: 0.0690\n",
      "Epoch 112/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2610 - acc: 0.0718 - val_loss: 4.2846 - val_acc: 0.0690\n",
      "Epoch 113/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2596 - acc: 0.0718 - val_loss: 4.2834 - val_acc: 0.0690\n",
      "Epoch 114/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2581 - acc: 0.0719 - val_loss: 4.2821 - val_acc: 0.0690\n",
      "Epoch 115/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2567 - acc: 0.0719 - val_loss: 4.2809 - val_acc: 0.0690\n",
      "Epoch 116/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2554 - acc: 0.0720 - val_loss: 4.2797 - val_acc: 0.0690\n",
      "Epoch 117/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2541 - acc: 0.0720 - val_loss: 4.2786 - val_acc: 0.0690\n",
      "Epoch 118/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2527 - acc: 0.0720 - val_loss: 4.2774 - val_acc: 0.0690\n",
      "Epoch 119/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2515 - acc: 0.0720 - val_loss: 4.2763 - val_acc: 0.0690\n",
      "Epoch 120/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2502 - acc: 0.0721 - val_loss: 4.2752 - val_acc: 0.0690\n",
      "Epoch 121/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2490 - acc: 0.0723 - val_loss: 4.2741 - val_acc: 0.0690\n",
      "Epoch 122/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2477 - acc: 0.0723 - val_loss: 4.2730 - val_acc: 0.0690\n",
      "Epoch 123/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2466 - acc: 0.0723 - val_loss: 4.2720 - val_acc: 0.0687\n",
      "Epoch 124/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2454 - acc: 0.0723 - val_loss: 4.2710 - val_acc: 0.0684\n",
      "Epoch 125/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2443 - acc: 0.0722 - val_loss: 4.2700 - val_acc: 0.0687\n",
      "Epoch 126/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2431 - acc: 0.0722 - val_loss: 4.2690 - val_acc: 0.0684\n",
      "Epoch 127/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2420 - acc: 0.0723 - val_loss: 4.2680 - val_acc: 0.0684\n",
      "Epoch 128/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2409 - acc: 0.0722 - val_loss: 4.2670 - val_acc: 0.0684\n",
      "Epoch 129/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2399 - acc: 0.0723 - val_loss: 4.2661 - val_acc: 0.0684\n",
      "Epoch 130/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2388 - acc: 0.0723 - val_loss: 4.2652 - val_acc: 0.0684\n",
      "Epoch 131/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2378 - acc: 0.0724 - val_loss: 4.2643 - val_acc: 0.0684\n",
      "Epoch 132/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2368 - acc: 0.0726 - val_loss: 4.2634 - val_acc: 0.0684\n",
      "Epoch 133/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2358 - acc: 0.0724 - val_loss: 4.2625 - val_acc: 0.0684\n",
      "Epoch 134/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2348 - acc: 0.0724 - val_loss: 4.2616 - val_acc: 0.0684\n",
      "Epoch 135/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2338 - acc: 0.0726 - val_loss: 4.2608 - val_acc: 0.0684\n",
      "Epoch 136/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2328 - acc: 0.0726 - val_loss: 4.2599 - val_acc: 0.0684\n",
      "Epoch 137/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2319 - acc: 0.0725 - val_loss: 4.2591 - val_acc: 0.0687\n",
      "Epoch 138/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2309 - acc: 0.0726 - val_loss: 4.2583 - val_acc: 0.0687\n",
      "Epoch 139/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2300 - acc: 0.0726 - val_loss: 4.2575 - val_acc: 0.0687\n",
      "Epoch 140/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2291 - acc: 0.0726 - val_loss: 4.2567 - val_acc: 0.0684\n",
      "Epoch 141/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2282 - acc: 0.0726 - val_loss: 4.2559 - val_acc: 0.0684\n",
      "Epoch 142/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2273 - acc: 0.0726 - val_loss: 4.2551 - val_acc: 0.0684\n",
      "Epoch 143/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2265 - acc: 0.0726 - val_loss: 4.2544 - val_acc: 0.0684\n",
      "Epoch 144/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2256 - acc: 0.0726 - val_loss: 4.2536 - val_acc: 0.0684\n",
      "Epoch 145/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2247 - acc: 0.0727 - val_loss: 4.2529 - val_acc: 0.0684\n",
      "Epoch 146/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2239 - acc: 0.0727 - val_loss: 4.2521 - val_acc: 0.0684\n",
      "Epoch 147/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2231 - acc: 0.0728 - val_loss: 4.2514 - val_acc: 0.0684\n",
      "Epoch 148/200\n",
      "13742/13742 [==============================] - 0s 27us/step - loss: 4.2223 - acc: 0.0728 - val_loss: 4.2507 - val_acc: 0.0684\n",
      "Epoch 149/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2214 - acc: 0.0726 - val_loss: 4.2500 - val_acc: 0.0684\n",
      "Epoch 150/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2206 - acc: 0.0726 - val_loss: 4.2493 - val_acc: 0.0684\n",
      "Epoch 151/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2198 - acc: 0.0728 - val_loss: 4.2486 - val_acc: 0.0684\n",
      "Epoch 152/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2191 - acc: 0.0727 - val_loss: 4.2479 - val_acc: 0.0684\n",
      "Epoch 153/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2183 - acc: 0.0727 - val_loss: 4.2472 - val_acc: 0.0684\n",
      "Epoch 154/200\n",
      "13742/13742 [==============================] - 0s 27us/step - loss: 4.2175 - acc: 0.0726 - val_loss: 4.2466 - val_acc: 0.0684\n",
      "Epoch 155/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2168 - acc: 0.0725 - val_loss: 4.2459 - val_acc: 0.0684\n",
      "Epoch 156/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2160 - acc: 0.0724 - val_loss: 4.2452 - val_acc: 0.0684\n",
      "Epoch 157/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2153 - acc: 0.0726 - val_loss: 4.2446 - val_acc: 0.0684\n",
      "Epoch 158/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2145 - acc: 0.0726 - val_loss: 4.2440 - val_acc: 0.0684\n",
      "Epoch 159/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2138 - acc: 0.0726 - val_loss: 4.2433 - val_acc: 0.0684\n",
      "Epoch 160/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2131 - acc: 0.0726 - val_loss: 4.2427 - val_acc: 0.0684\n",
      "Epoch 161/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2124 - acc: 0.0725 - val_loss: 4.2421 - val_acc: 0.0684\n",
      "Epoch 162/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2117 - acc: 0.0725 - val_loss: 4.2415 - val_acc: 0.0681\n",
      "Epoch 163/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2110 - acc: 0.0726 - val_loss: 4.2409 - val_acc: 0.0681\n",
      "Epoch 164/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2103 - acc: 0.0727 - val_loss: 4.2403 - val_acc: 0.0681\n",
      "Epoch 165/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2096 - acc: 0.0726 - val_loss: 4.2397 - val_acc: 0.0681\n",
      "Epoch 166/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2089 - acc: 0.0727 - val_loss: 4.2391 - val_acc: 0.0681\n",
      "Epoch 167/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2083 - acc: 0.0725 - val_loss: 4.2385 - val_acc: 0.0681\n",
      "Epoch 168/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2076 - acc: 0.0726 - val_loss: 4.2379 - val_acc: 0.0684\n",
      "Epoch 169/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2069 - acc: 0.0728 - val_loss: 4.2374 - val_acc: 0.0684\n",
      "Epoch 170/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2063 - acc: 0.0730 - val_loss: 4.2368 - val_acc: 0.0684\n",
      "Epoch 171/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2056 - acc: 0.0727 - val_loss: 4.2363 - val_acc: 0.0684\n",
      "Epoch 172/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2050 - acc: 0.0728 - val_loss: 4.2357 - val_acc: 0.0684\n",
      "Epoch 173/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2043 - acc: 0.0728 - val_loss: 4.2352 - val_acc: 0.0684\n",
      "Epoch 174/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2037 - acc: 0.0729 - val_loss: 4.2346 - val_acc: 0.0684\n",
      "Epoch 175/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2031 - acc: 0.0728 - val_loss: 4.2341 - val_acc: 0.0681\n",
      "Epoch 176/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2025 - acc: 0.0728 - val_loss: 4.2336 - val_acc: 0.0681\n",
      "Epoch 177/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2018 - acc: 0.0729 - val_loss: 4.2330 - val_acc: 0.0678\n",
      "Epoch 178/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2012 - acc: 0.0730 - val_loss: 4.2325 - val_acc: 0.0678\n",
      "Epoch 179/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2006 - acc: 0.0730 - val_loss: 4.2320 - val_acc: 0.0678\n",
      "Epoch 180/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.2000 - acc: 0.0730 - val_loss: 4.2315 - val_acc: 0.0678\n",
      "Epoch 181/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1994 - acc: 0.0729 - val_loss: 4.2310 - val_acc: 0.0678\n",
      "Epoch 182/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1988 - acc: 0.0731 - val_loss: 4.2305 - val_acc: 0.0678\n",
      "Epoch 183/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1983 - acc: 0.0730 - val_loss: 4.2300 - val_acc: 0.0678\n",
      "Epoch 184/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1977 - acc: 0.0729 - val_loss: 4.2295 - val_acc: 0.0678\n",
      "Epoch 185/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1971 - acc: 0.0731 - val_loss: 4.2290 - val_acc: 0.0678\n",
      "Epoch 186/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1965 - acc: 0.0730 - val_loss: 4.2285 - val_acc: 0.0681\n",
      "Epoch 187/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1959 - acc: 0.0731 - val_loss: 4.2280 - val_acc: 0.0678\n",
      "Epoch 188/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1954 - acc: 0.0731 - val_loss: 4.2275 - val_acc: 0.0678\n",
      "Epoch 189/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1948 - acc: 0.0729 - val_loss: 4.2270 - val_acc: 0.0681\n",
      "Epoch 190/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1943 - acc: 0.0729 - val_loss: 4.2266 - val_acc: 0.0681\n",
      "Epoch 191/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1937 - acc: 0.0729 - val_loss: 4.2261 - val_acc: 0.0681\n",
      "Epoch 192/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1931 - acc: 0.0731 - val_loss: 4.2256 - val_acc: 0.0681\n",
      "Epoch 193/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1926 - acc: 0.0731 - val_loss: 4.2251 - val_acc: 0.0681\n",
      "Epoch 194/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1921 - acc: 0.0731 - val_loss: 4.2247 - val_acc: 0.0684\n",
      "Epoch 195/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1915 - acc: 0.0730 - val_loss: 4.2242 - val_acc: 0.0684\n",
      "Epoch 196/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1910 - acc: 0.0730 - val_loss: 4.2238 - val_acc: 0.0684\n",
      "Epoch 197/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1904 - acc: 0.0730 - val_loss: 4.2233 - val_acc: 0.0684\n",
      "Epoch 198/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1899 - acc: 0.0731 - val_loss: 4.2228 - val_acc: 0.0684\n",
      "Epoch 199/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1894 - acc: 0.0730 - val_loss: 4.2224 - val_acc: 0.0684\n",
      "Epoch 200/200\n",
      "13742/13742 [==============================] - 0s 28us/step - loss: 4.1888 - acc: 0.0730 - val_loss: 4.2219 - val_acc: 0.0684\n",
      "5726/5726 [==============================] - 0s 40us/step\n",
      "\n",
      "Test loss: 4.230476000156596\n",
      "\n",
      "Test accuracy: 0.06880894168779395\n"
     ]
    }
   ],
   "source": [
    "net = my_Net()\n",
    " \n",
    "net.make_model()\n",
    " \n",
    "net.train(X_train, y_train)\n",
    "    \n",
    "score = net.score(X_test, y_test)\n",
    " \n",
    "print(\"\\nTest loss:\", score[0])\n",
    "print(\"\\nTest accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルを保存\n",
    "model_path=os.path.join(os.environ['BR_HOME'],\"boatrace/models\")\n",
    "model_modelfile=os.path.join(model_path,'keras_perceptron_model.json')\n",
    "model_weightfile=os.path.join(model_path,'keras_perceptron_weight.json')\n",
    "\n",
    "open(model_modelfile,\"w\").write(net.model.to_json())\n",
    "net.model.save_weights(model_weightfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy: 20190220-13-07 4 0 49.6 0.092\n",
      "buy: 20190220-13-07 4 5 25.4 0.08\n",
      "buy: 20190514-10-09 7 0 94.9 0.107\n",
      "buy: 20190317-14-01 8 0 17.8 0.12\n",
      "buy: 20190317-11-08 10 0 23.8 0.084\n",
      "buy: 20190425-06-03 13 0 21.4 0.084\n",
      "buy: 20190509-15-03 21 0 24.3 0.086\n",
      "buy: 20190103-22-06 31 0 45.5 0.091\n",
      "buy: 20190418-01-02 35 0 56.6 0.101\n",
      "buy: 20190418-01-02 35 1 35.4 0.089\n",
      "buy: 20190313-03-11 36 1 29.4 0.084\n",
      "buy: 20190527-06-05 45 0 19.0 0.206\n",
      "buy: 20190415-09-09 51 0 31.5 0.097\n",
      "☆hit!☆: 20190415-09-09 0 31.5\n",
      "buy: 20190415-09-09 51 1 23.0 0.092\n",
      "buy: 20190419-19-02 52 0 23.9 0.106\n",
      "buy: 20190419-19-02 52 1 40.9 0.091\n",
      "buy: 20190417-12-12 60 0 43.3 0.149\n",
      "buy: 20190417-12-12 60 1 33.7 0.147\n",
      "☆hit!☆: 20190417-12-12 1 33.7\n",
      "buy: 20190404-15-06 61 0 31.4 0.103\n",
      "buy: 20190502-01-10 66 0 22.0 0.109\n",
      "buy: 20190119-07-03 75 0 22.0 0.092\n",
      "buy: 20190203-11-09 76 0 43.1 0.093\n",
      "buy: 20190105-05-01 78 0 22.7 0.081\n",
      "buy: 20190423-11-11 79 0 16.0 0.119\n",
      "buy: 20190106-03-11 80 0 11.9 0.127\n",
      "buy: 20190412-09-03 86 0 22.0 0.081\n",
      "buy: 20190221-03-02 92 0 64.7 0.136\n",
      "buy: 20190415-04-03 97 0 76.7 0.123\n",
      "buy: 20190217-17-09 99 0 18.1 0.11\n",
      "buy: 20190223-10-10 106 0 34.0 0.097\n",
      "buy: 20190407-04-09 109 0 19.4 0.107\n",
      "buy: 20190523-10-09 110 0 14.6 0.141\n",
      "buy: 20190525-15-08 111 0 12.5 0.14\n",
      "buy: 20190519-19-07 113 0 67.2 0.082\n",
      "buy: 20190427-22-02 114 0 135.6 0.098\n",
      "☆hit!☆: 20190427-22-02 0 135.6\n",
      "buy: 20190213-03-11 123 0 64.7 0.118\n",
      "☆hit!☆: 20190213-03-11 0 64.7\n",
      "buy: 20190213-03-11 123 1 69.5 0.082\n",
      "buy: 20190505-15-09 124 0 84.6 0.102\n",
      "buy: 20190416-20-06 126 0 12.4 0.138\n",
      "buy: 20190416-20-06 126 1 26.3 0.105\n",
      "buy: 20190309-24-08 129 0 79.8 0.127\n",
      "☆hit!☆: 20190309-24-08 0 79.8\n",
      "buy: 20190309-24-08 129 1 162.5 0.085\n",
      "buy: 20190315-17-01 133 0 30.3 0.142\n",
      "buy: 20190103-22-07 135 0 35.1 0.087\n",
      "buy: 20190209-18-10 141 0 28.5 0.138\n",
      "buy: 20190209-18-10 141 1 44.6 0.083\n",
      "buy: 20190416-07-01 183 0 17.3 0.088\n",
      "buy: 20190119-14-01 190 0 18.9 0.104\n",
      "buy: 20190521-19-08 192 0 31.4 0.099\n",
      "buy: 20190315-05-05 198 0 99.9 0.093\n",
      "buy: 20190323-01-02 199 1 26.0 0.092\n",
      "buy: 20190530-12-10 200 0 33.0 0.132\n",
      "buy: 20190530-12-10 200 1 16.8 0.103\n",
      "buy: 20190515-05-04 204 0 35.0 0.083\n",
      "buy: 20190130-21-03 215 0 20.3 0.115\n",
      "buy: 20190108-04-03 216 0 118.2 0.091\n",
      "buy: 20190119-09-11 218 0 19.5 0.126\n",
      "buy: 20190317-22-09 222 0 20.9 0.099\n",
      "☆hit!☆: 20190317-22-09 0 20.9\n",
      "buy: 20190405-04-10 230 0 44.0 0.087\n",
      "buy: 20190503-07-10 236 0 15.6 0.136\n",
      "buy: 20190503-07-10 236 1 16.0 0.095\n",
      "buy: 20190505-04-06 242 0 25.8 0.086\n",
      "☆hit!☆: 20190505-04-06 0 25.8\n",
      "buy: 20190505-04-06 242 1 110.5 0.096\n",
      "buy: 20190106-22-03 251 0 29.3 0.109\n",
      "buy: 20190313-08-03 253 0 25.3 0.111\n",
      "buy: 20190211-03-01 255 0 15.7 0.124\n",
      "buy: 20190116-21-01 260 0 22.6 0.084\n",
      "☆hit!☆: 20190116-21-01 0 22.6\n",
      "buy: 20190123-10-11 271 0 21.4 0.083\n",
      "buy: 20190303-12-03 279 0 41.3 0.111\n",
      "buy: 20190405-13-01 280 0 19.2 0.117\n",
      "buy: 20190304-12-03 284 0 60.4 0.116\n",
      "buy: 20190512-17-06 289 0 15.8 0.118\n",
      "☆hit!☆: 20190512-17-06 0 15.8\n",
      "buy: 20190121-04-12 303 0 21.3 0.092\n",
      "☆hit!☆: 20190121-04-12 0 21.3\n",
      "buy: 20190513-09-09 304 0 16.3 0.1\n",
      "buy: 20190513-09-09 304 5 30.7 0.089\n",
      "buy: 20190512-06-08 308 0 16.1 0.124\n",
      "buy: 20190326-16-07 314 0 62.8 0.127\n",
      "☆hit!☆: 20190326-16-07 0 62.8\n",
      "buy: 20190326-16-07 314 1 132.6 0.099\n",
      "buy: 20190417-08-08 336 0 65.7 0.117\n",
      "buy: 20190422-02-03 338 0 25.1 0.102\n",
      "buy: 20190111-16-03 344 0 13.7 0.131\n",
      "buy: 20190111-16-03 344 1 49.2 0.086\n",
      "buy: 20190327-20-09 347 0 21.9 0.086\n",
      "buy: 20190126-10-06 355 0 45.2 0.108\n",
      "buy: 20190126-10-06 355 1 24.3 0.095\n",
      "buy: 20190108-24-04 364 0 16.7 0.108\n",
      "buy: 20190519-02-01 370 1 30.5 0.094\n",
      "buy: 20190520-18-11 382 0 22.3 0.091\n",
      "buy: 20190403-07-08 387 0 17.2 0.091\n",
      "buy: 20190506-21-08 390 0 15.7 0.127\n",
      "☆hit!☆: 20190506-21-08 0 15.7\n",
      "buy: 20190210-02-02 391 0 66.6 0.088\n",
      "buy: 20190510-17-02 414 0 40.6 0.1\n",
      "buy: 20190107-12-05 419 0 26.9 0.108\n",
      "buy: 20190201-21-02 420 0 94.6 0.082\n",
      "buy: 20190325-07-11 423 0 18.8 0.111\n",
      "buy: 20190131-08-11 429 0 15.9 0.108\n",
      "buy: 20190306-02-08 450 0 26.9 0.082\n",
      "buy: 20190113-01-05 454 0 18.6 0.102\n",
      "buy: 20190204-08-06 457 0 23.5 0.087\n",
      "buy: 20190204-08-06 457 1 32.5 0.096\n",
      "buy: 20190113-08-09 464 0 17.6 0.091\n",
      "buy: 20190329-04-09 465 0 18.6 0.09\n",
      "buy: 20190407-09-03 466 1 20.5 0.088\n",
      "buy: 20190109-09-05 468 0 12.7 0.14\n",
      "buy: 20190514-16-08 469 0 66.6 0.082\n",
      "buy: 20190327-22-02 486 0 33.4 0.196\n",
      "buy: 20190327-22-02 486 1 80.7 0.092\n",
      "buy: 20190327-22-02 486 4 17.5 0.094\n",
      "buy: 20190514-22-06 491 0 44.4 0.109\n",
      "buy: 20190303-03-05 494 0 24.8 0.084\n",
      "buy: 20190508-13-02 497 0 43.1 0.197\n",
      "buy: 20190106-02-02 499 0 84.2 0.098\n",
      "buy: 20190427-17-08 504 0 23.2 0.1\n",
      "buy: 20190225-05-04 505 0 35.1 0.096\n",
      "buy: 20190227-11-03 514 0 37.2 0.098\n",
      "buy: 20190403-21-02 519 0 37.8 0.092\n",
      "buy: 20190117-15-03 524 0 30.7 0.081\n",
      "☆hit!☆: 20190117-15-03 0 30.7\n",
      "buy: 20190224-21-08 525 0 24.4 0.093\n",
      "buy: 20190317-16-05 531 0 25.6 0.095\n",
      "buy: 20190519-07-03 533 0 19.1 0.097\n",
      "☆hit!☆: 20190519-07-03 0 19.1\n",
      "buy: 20190321-01-05 536 0 27.9 0.09\n",
      "buy: 20190117-06-01 537 0 22.5 0.133\n",
      "☆hit!☆: 20190117-06-01 0 22.5\n",
      "buy: 20190110-07-04 539 0 197.3 0.105\n",
      "☆hit!☆: 20190110-07-04 0 197.3\n",
      "buy: 20190110-07-04 539 1 142.0 0.08\n",
      "buy: 20190318-20-05 548 0 27.6 0.083\n",
      "buy: 20190112-07-05 560 0 148.6 0.106\n",
      "buy: 20190112-07-05 560 1 116.2 0.099\n",
      "☆hit!☆: 20190112-07-05 1 116.2\n",
      "buy: 20190531-02-05 561 0 29.7 0.085\n",
      "buy: 20190405-06-02 567 0 29.9 0.101\n",
      "buy: 20190405-06-02 567 1 50.8 0.082\n",
      "buy: 20190405-06-02 567 5 154.0 0.102\n",
      "buy: 20190420-01-04 574 0 23.8 0.107\n",
      "buy: 20190319-19-03 575 0 21.4 0.09\n",
      "buy: 20190305-15-10 578 0 23.7 0.104\n",
      "buy: 20190212-24-02 582 1 24.6 0.089\n",
      "buy: 20190320-22-01 586 0 25.1 0.091\n",
      "☆hit!☆: 20190320-22-01 0 25.1\n",
      "buy: 20190304-21-09 588 1 78.6 0.088\n",
      "☆hit!☆: 20190304-21-09 1 78.6\n",
      "buy: 20190520-06-05 594 0 19.2 0.08\n",
      "buy: 20190126-18-05 599 0 22.5 0.091\n",
      "buy: 20190330-02-03 603 0 57.4 0.084\n",
      "buy: 20190420-18-10 604 0 20.3 0.097\n",
      "buy: 20190114-09-04 605 0 19.2 0.086\n",
      "buy: 20190524-06-02 612 0 67.7 0.106\n",
      "buy: 20190104-02-03 615 0 17.2 0.092\n",
      "buy: 20190108-04-07 621 0 28.6 0.165\n",
      "buy: 20190108-04-07 621 1 88.6 0.13\n",
      "buy: 20190105-16-07 622 0 36.2 0.089\n",
      "buy: 20190111-09-09 623 1 43.2 0.082\n",
      "☆hit!☆: 20190111-09-09 1 43.2\n",
      "buy: 20190127-19-04 625 0 25.6 0.099\n",
      "buy: 20190430-07-05 628 1 20.6 0.081\n",
      "buy: 20190111-13-05 631 0 43.8 0.149\n",
      "buy: 20190225-15-02 636 0 20.8 0.102\n",
      "☆hit!☆: 20190225-15-02 0 20.8\n",
      "buy: 20190101-18-07 637 0 26.0 0.109\n",
      "☆hit!☆: 20190101-18-07 0 26.0\n",
      "buy: 20190101-18-07 637 4 37.1 0.082\n",
      "buy: 20190507-13-05 642 0 21.3 0.109\n",
      "buy: 20190507-13-05 642 1 50.0 0.09\n",
      "buy: 20190429-22-06 655 0 27.5 0.1\n",
      "buy: 20190201-07-11 658 0 18.9 0.089\n",
      "buy: 20190316-02-01 659 0 19.8 0.089\n",
      "buy: 20190102-16-09 669 0 35.6 0.082\n",
      "buy: 20190415-23-09 713 0 24.4 0.083\n",
      "buy: 20190510-24-08 715 0 12.0 0.133\n",
      "buy: 20190309-20-11 717 0 18.0 0.098\n",
      "buy: 20190309-20-11 717 1 18.4 0.088\n",
      "buy: 20190426-08-04 718 0 30.5 0.091\n",
      "buy: 20190107-04-02 729 0 18.1 0.1\n",
      "buy: 20190317-08-06 734 0 101.4 0.119\n",
      "buy: 20190225-16-04 741 1 21.7 0.082\n",
      "buy: 20190308-16-01 745 0 38.0 0.084\n",
      "buy: 20190223-13-01 747 0 18.3 0.106\n",
      "buy: 20190424-13-04 759 0 41.0 0.08\n",
      "buy: 20190324-05-05 760 1 47.0 0.082\n",
      "buy: 20190104-03-08 761 0 34.6 0.093\n",
      "buy: 20190316-17-03 767 0 18.4 0.094\n",
      "buy: 20190401-10-07 768 0 20.9 0.098\n",
      "buy: 20190516-02-11 775 0 19.5 0.115\n",
      "☆hit!☆: 20190516-02-11 0 19.5\n",
      "buy: 20190516-02-11 775 1 24.5 0.08\n",
      "buy: 20190407-04-06 777 0 21.9 0.092\n",
      "buy: 20190114-11-07 779 0 78.2 0.085\n",
      "buy: 20190107-16-09 780 0 21.1 0.099\n",
      "buy: 20190323-16-10 784 0 20.4 0.089\n",
      "buy: 20190110-24-01 800 0 35.7 0.09\n",
      "buy: 20190509-18-05 801 0 13.5 0.127\n",
      "buy: 20190222-10-11 805 1 36.5 0.083\n",
      "buy: 20190512-20-03 808 0 49.8 0.088\n",
      "buy: 20190413-20-05 814 0 13.6 0.129\n",
      "buy: 20190113-12-03 833 0 27.7 0.098\n",
      "buy: 20190214-15-07 834 0 71.0 0.106\n",
      "buy: 20190211-08-05 835 0 16.4 0.096\n",
      "buy: 20190117-17-06 837 0 34.2 0.137\n",
      "☆hit!☆: 20190117-17-06 0 34.2\n",
      "buy: 20190528-09-09 838 0 85.2 0.091\n",
      "buy: 20190528-09-09 838 1 43.9 0.092\n",
      "buy: 20190210-07-06 840 0 126.1 0.093\n",
      "buy: 20190101-06-09 845 4 21.5 0.083\n",
      "buy: 20190428-05-05 851 0 27.1 0.095\n",
      "buy: 20190429-13-05 854 0 31.8 0.146\n",
      "buy: 20190429-13-05 854 1 119.8 0.091\n",
      "buy: 20190221-19-07 865 0 16.1 0.138\n",
      "buy: 20190121-13-03 872 1 39.3 0.099\n",
      "buy: 20190507-13-07 884 0 14.1 0.109\n",
      "buy: 20190315-08-08 885 0 115.0 0.125\n",
      "buy: 20190308-16-06 908 0 36.0 0.089\n",
      "buy: 20190308-16-06 908 4 22.5 0.085\n",
      "buy: 20190505-02-05 916 0 17.5 0.101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy: 20190414-20-06 920 0 25.9 0.091\n",
      "buy: 20190112-20-03 924 0 17.6 0.12\n",
      "buy: 20190214-14-03 930 0 15.1 0.101\n",
      "buy: 20190421-24-04 935 1 19.9 0.084\n",
      "buy: 20190421-24-04 935 5 67.2 0.086\n",
      "buy: 20190402-15-12 941 0 22.6 0.081\n",
      "buy: 20190504-19-06 942 0 96.5 0.095\n",
      "buy: 20190309-15-06 949 0 41.0 0.099\n",
      "buy: 20190430-13-07 951 1 45.4 0.091\n",
      "buy: 20190211-14-02 965 0 42.8 0.092\n",
      "buy: 20190427-07-03 966 0 15.3 0.118\n",
      "buy: 20190104-07-08 970 1 43.3 0.103\n",
      "buy: 20190326-22-05 973 1 28.6 0.099\n",
      "buy: 20190516-16-01 976 0 19.3 0.089\n",
      "buy: 20190513-22-09 978 0 33.0 0.094\n",
      "buy: 20190405-07-07 981 0 29.9 0.081\n",
      "buy: 20190212-21-10 983 0 46.5 0.14\n",
      "buy: 20190212-21-10 983 1 25.0 0.086\n",
      "buy: 20190329-22-03 992 0 16.7 0.102\n",
      "buy: 20190329-22-03 992 1 46.4 0.089\n",
      "buy: 20190505-10-07 1010 0 26.1 0.121\n",
      "buy: 20190505-10-07 1010 1 36.8 0.086\n",
      "☆hit!☆: 20190505-10-07 1 36.8\n",
      "buy: 20190311-03-01 1018 0 15.0 0.116\n",
      "buy: 20190311-03-01 1018 1 35.6 0.099\n",
      "buy: 20190302-06-03 1019 0 34.0 0.113\n",
      "buy: 20190318-16-07 1022 0 64.3 0.132\n",
      "buy: 20190209-23-05 1023 0 50.4 0.085\n",
      "buy: 20190305-02-04 1026 0 18.3 0.101\n",
      "buy: 20190224-08-06 1028 0 57.2 0.111\n",
      "buy: 20190224-08-06 1028 1 52.0 0.143\n",
      "buy: 20190101-09-07 1031 0 47.6 0.125\n",
      "buy: 20190101-09-07 1031 1 50.2 0.087\n",
      "buy: 20190311-21-06 1043 0 17.1 0.108\n",
      "buy: 20190103-05-11 1048 0 37.8 0.111\n",
      "buy: 20190423-11-08 1050 0 33.6 0.126\n",
      "buy: 20190329-14-09 1068 0 100.9 0.093\n",
      "buy: 20190126-01-12 1078 1 26.3 0.089\n",
      "buy: 20190114-01-05 1083 0 61.5 0.11\n",
      "buy: 20190519-02-04 1087 0 29.5 0.086\n",
      "buy: 20190216-06-03 1089 0 24.2 0.095\n",
      "buy: 20190411-04-04 1101 0 34.0 0.093\n",
      "☆hit!☆: 20190411-04-04 0 34.0\n",
      "buy: 20190411-04-04 1101 1 86.4 0.084\n",
      "buy: 20190219-01-03 1107 1 48.4 0.092\n",
      "buy: 20190105-13-07 1110 0 13.0 0.151\n",
      "buy: 20190322-01-07 1111 0 18.4 0.09\n",
      "buy: 20190321-01-01 1114 0 47.0 0.081\n",
      "buy: 20190321-22-10 1118 0 20.0 0.104\n",
      "buy: 20190321-22-10 1118 5 27.3 0.085\n",
      "buy: 20190528-09-06 1120 0 122.6 0.088\n",
      "buy: 20190209-24-06 1139 0 20.6 0.085\n",
      "buy: 20190209-24-06 1139 1 33.4 0.102\n",
      "buy: 20190214-16-01 1149 0 52.7 0.104\n",
      "buy: 20190501-23-09 1150 0 33.1 0.105\n",
      "buy: 20190503-10-10 1162 0 28.2 0.082\n",
      "buy: 20190103-12-02 1165 0 29.5 0.142\n",
      "buy: 20190103-12-02 1165 1 34.5 0.086\n",
      "buy: 20190418-12-04 1169 0 25.2 0.107\n",
      "buy: 20190224-05-03 1170 0 24.1 0.081\n",
      "buy: 20190427-13-07 1178 0 50.8 0.099\n",
      "buy: 20190326-24-09 1181 0 11.6 0.138\n",
      "☆hit!☆: 20190326-24-09 0 11.6\n",
      "buy: 20190326-24-09 1181 1 26.1 0.102\n",
      "buy: 20190103-02-02 1183 0 103.8 0.113\n",
      "buy: 20190119-06-10 1187 0 33.1 0.094\n",
      "buy: 20190425-23-09 1193 0 21.4 0.096\n",
      "buy: 20190504-03-04 1204 0 26.9 0.098\n",
      "☆hit!☆: 20190504-03-04 0 26.9\n",
      "buy: 20190116-15-10 1208 0 49.8 0.092\n",
      "buy: 20190104-24-05 1209 0 53.0 0.128\n",
      "buy: 20190106-01-08 1210 0 14.7 0.106\n",
      "buy: 20190304-14-06 1211 0 36.9 0.106\n",
      "buy: 20190316-13-07 1219 1 26.2 0.084\n",
      "buy: 20190215-04-10 1225 0 12.4 0.162\n",
      "buy: 20190203-14-06 1232 0 22.4 0.084\n",
      "buy: 20190331-08-07 1235 0 44.9 0.094\n",
      "buy: 20190331-08-07 1235 1 183.3 0.082\n",
      "buy: 20190425-20-02 1238 0 17.7 0.145\n",
      "buy: 20190417-17-06 1242 0 25.6 0.104\n",
      "buy: 20190120-04-04 1244 0 78.5 0.092\n",
      "buy: 20190209-19-04 1251 0 22.8 0.129\n",
      "buy: 20190514-09-07 1252 0 12.0 0.132\n",
      "buy: 20190511-02-06 1257 0 19.5 0.081\n",
      "buy: 20190320-03-06 1259 0 92.5 0.098\n",
      "buy: 20190415-09-08 1262 1 88.9 0.082\n",
      "buy: 20190521-01-04 1266 0 90.8 0.109\n",
      "buy: 20190520-07-04 1271 0 13.5 0.115\n",
      "buy: 20190531-17-09 1276 0 36.2 0.139\n",
      "buy: 20190518-11-11 1285 0 18.4 0.089\n",
      "☆hit!☆: 20190518-11-11 0 18.4\n",
      "buy: 20190228-04-12 1290 0 11.2 0.143\n",
      "buy: 20190204-02-08 1292 0 16.4 0.103\n",
      "buy: 20190325-11-06 1294 0 29.0 0.093\n",
      "buy: 20190419-01-10 1298 0 22.0 0.105\n",
      "buy: 20190416-19-04 1320 0 26.6 0.083\n",
      "buy: 20190414-20-11 1326 0 19.4 0.087\n",
      "☆hit!☆: 20190414-20-11 0 19.4\n",
      "buy: 20190418-23-12 1330 0 11.5 0.133\n",
      "buy: 20190206-01-02 1342 0 26.1 0.168\n",
      "buy: 20190206-01-02 1342 1 40.1 0.088\n",
      "buy: 20190105-11-07 1345 0 42.4 0.092\n",
      "buy: 20190105-11-07 1345 1 93.0 0.096\n",
      "buy: 20190303-11-03 1348 0 29.8 0.119\n",
      "buy: 20190115-01-07 1354 0 54.0 0.088\n",
      "buy: 20190422-24-02 1360 1 23.2 0.085\n",
      "☆hit!☆: 20190422-24-02 1 23.2\n",
      "buy: 20190127-02-05 1367 0 28.8 0.116\n",
      "buy: 20190309-02-02 1368 0 55.0 0.09\n",
      "buy: 20190309-20-12 1373 0 20.6 0.127\n",
      "buy: 20190511-24-02 1382 0 31.1 0.082\n",
      "buy: 20190108-20-09 1389 1 28.4 0.103\n",
      "buy: 20190111-17-05 1392 1 31.4 0.087\n",
      "buy: 20190102-07-10 1398 0 43.5 0.088\n",
      "buy: 20190114-10-03 1400 0 24.0 0.081\n",
      "buy: 20190331-14-01 1402 0 20.5 0.113\n",
      "☆hit!☆: 20190331-14-01 0 20.5\n",
      "buy: 20190109-24-01 1411 0 19.6 0.12\n",
      "buy: 20190313-21-02 1419 0 24.3 0.092\n",
      "buy: 20190130-16-05 1423 0 30.4 0.086\n",
      "buy: 20190425-16-08 1427 0 24.9 0.099\n",
      "buy: 20190215-01-06 1428 1 18.5 0.092\n",
      "buy: 20190210-02-06 1429 1 27.8 0.083\n",
      "buy: 20190210-02-06 1429 5 95.1 0.082\n",
      "buy: 20190314-05-01 1435 0 22.3 0.081\n",
      "buy: 20190106-14-06 1438 0 41.7 0.084\n",
      "buy: 20190101-15-03 1452 0 57.2 0.104\n",
      "buy: 20190529-23-09 1457 0 41.1 0.094\n",
      "buy: 20190304-13-11 1461 0 13.2 0.115\n",
      "☆hit!☆: 20190304-13-11 0 13.2\n",
      "buy: 20190526-01-02 1462 0 23.0 0.082\n",
      "buy: 20190326-23-07 1468 0 66.1 0.085\n",
      "buy: 20190214-09-10 1480 0 16.4 0.1\n",
      "buy: 20190306-17-12 1487 0 16.8 0.098\n",
      "buy: 20190520-07-05 1488 0 20.2 0.109\n",
      "buy: 20190426-08-01 1490 0 14.9 0.143\n",
      "buy: 20190426-08-01 1490 1 21.1 0.081\n",
      "buy: 20190227-22-12 1503 0 39.0 0.087\n",
      "buy: 20190315-21-06 1504 0 34.2 0.095\n",
      "buy: 20190501-05-04 1506 0 24.3 0.136\n",
      "buy: 20190112-10-03 1508 0 25.3 0.083\n",
      "buy: 20190321-13-05 1509 0 44.1 0.083\n",
      "buy: 20190303-12-04 1510 0 38.5 0.088\n",
      "buy: 20190415-22-05 1520 0 95.5 0.188\n",
      "buy: 20190415-22-05 1520 1 186.7 0.098\n",
      "buy: 20190409-19-11 1523 0 13.0 0.123\n",
      "buy: 20190314-14-04 1528 0 14.0 0.137\n",
      "buy: 20190314-14-04 1528 1 34.1 0.109\n",
      "buy: 20190425-07-09 1533 0 64.3 0.089\n",
      "buy: 20190408-18-02 1534 0 40.8 0.104\n",
      "buy: 20190408-18-02 1534 1 19.8 0.106\n",
      "☆hit!☆: 20190408-18-02 1 19.8\n",
      "buy: 20190102-14-02 1537 0 68.5 0.144\n",
      "buy: 20190102-14-02 1537 1 156.8 0.116\n",
      "buy: 20190317-22-04 1561 0 15.2 0.106\n",
      "buy: 20190408-01-08 1562 0 27.8 0.092\n",
      "buy: 20190408-01-08 1562 1 53.4 0.08\n",
      "buy: 20190415-22-01 1578 1 22.5 0.081\n",
      "buy: 20190318-14-09 1581 0 16.2 0.115\n",
      "buy: 20190318-14-09 1581 1 20.3 0.109\n",
      "☆hit!☆: 20190318-14-09 1 20.3\n",
      "buy: 20190318-14-09 1581 5 47.0 0.086\n",
      "buy: 20190521-24-03 1582 1 27.3 0.095\n",
      "buy: 20190530-20-04 1585 0 62.6 0.15\n",
      "buy: 20190530-20-04 1585 1 55.5 0.11\n",
      "buy: 20190507-05-02 1593 0 73.0 0.095\n",
      "buy: 20190513-09-04 1597 0 49.1 0.083\n",
      "buy: 20190304-14-08 1599 0 83.7 0.123\n",
      "buy: 20190304-14-08 1599 4 158.7 0.08\n",
      "buy: 20190312-05-05 1605 0 13.9 0.113\n",
      "buy: 20190308-15-03 1606 0 80.9 0.083\n",
      "buy: 20190102-03-03 1610 0 69.6 0.099\n",
      "buy: 20190107-17-04 1613 0 30.1 0.087\n",
      "buy: 20190421-06-03 1615 0 30.2 0.082\n",
      "buy: 20190317-22-02 1618 0 10.7 0.148\n",
      "buy: 20190120-07-08 1626 1 15.2 0.127\n",
      "buy: 20190529-07-06 1627 0 32.2 0.116\n",
      "buy: 20190526-06-06 1628 0 63.0 0.194\n",
      "buy: 20190526-06-06 1628 1 26.6 0.095\n",
      "buy: 20190407-23-09 1634 0 18.3 0.088\n",
      "buy: 20190426-21-07 1646 0 33.1 0.205\n",
      "buy: 20190426-21-07 1646 1 64.7 0.118\n",
      "buy: 20190325-12-03 1663 0 20.8 0.104\n",
      "☆hit!☆: 20190325-12-03 0 20.8\n",
      "buy: 20190219-01-10 1668 0 16.0 0.111\n",
      "buy: 20190424-04-04 1669 0 46.5 0.092\n",
      "buy: 20190424-04-04 1669 1 48.4 0.081\n",
      "buy: 20190304-10-05 1672 0 37.0 0.105\n",
      "buy: 20190222-14-09 1679 0 35.0 0.082\n",
      "buy: 20190202-24-07 1683 0 20.9 0.199\n",
      "buy: 20190405-19-06 1691 0 18.6 0.087\n",
      "buy: 20190316-22-03 1694 0 17.7 0.103\n",
      "buy: 20190316-22-03 1694 1 32.3 0.103\n",
      "buy: 20190504-08-08 1699 0 74.6 0.095\n",
      "buy: 20190504-08-08 1699 1 243.5 0.089\n",
      "buy: 20190111-15-07 1703 0 66.2 0.085\n",
      "buy: 20190119-19-01 1706 0 16.9 0.143\n",
      "buy: 20190119-19-01 1706 1 27.9 0.104\n",
      "buy: 20190423-18-08 1714 0 45.9 0.114\n",
      "buy: 20190527-14-06 1716 0 61.0 0.108\n",
      "buy: 20190403-09-07 1723 0 20.4 0.08\n",
      "buy: 20190403-09-07 1723 1 46.6 0.085\n",
      "☆hit!☆: 20190403-09-07 1 46.6\n",
      "buy: 20190304-11-02 1725 0 59.7 0.12\n",
      "buy: 20190310-12-02 1726 0 24.4 0.104\n",
      "☆hit!☆: 20190310-12-02 0 24.4\n",
      "buy: 20190503-08-06 1727 0 64.0 0.083\n",
      "buy: 20190129-14-07 1747 0 29.0 0.115\n",
      "buy: 20190129-14-07 1747 1 64.0 0.097\n",
      "buy: 20190129-14-07 1747 5 41.9 0.091\n",
      "buy: 20190319-10-06 1755 0 37.6 0.1\n",
      "buy: 20190319-10-06 1755 1 18.6 0.099\n",
      "buy: 20190419-07-11 1757 0 32.6 0.082\n",
      "buy: 20190319-14-07 1759 1 23.5 0.085\n",
      "buy: 20190407-05-09 1765 0 20.3 0.125\n",
      "buy: 20190102-07-05 1788 0 44.1 0.083\n",
      "buy: 20190407-07-07 1798 0 24.5 0.084\n",
      "buy: 20190413-04-07 1800 0 27.8 0.114\n",
      "buy: 20190421-02-10 1806 0 32.3 0.085\n",
      "buy: 20190103-02-10 1807 0 30.5 0.097\n",
      "buy: 20190103-02-10 1807 1 28.7 0.081\n",
      "☆hit!☆: 20190103-02-10 1 28.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy: 20190509-20-02 1813 0 24.7 0.135\n",
      "buy: 20190331-17-02 1814 0 19.3 0.121\n",
      "buy: 20190408-09-09 1816 1 16.0 0.109\n",
      "buy: 20190323-12-01 1826 0 22.7 0.086\n",
      "buy: 20190316-05-04 1829 0 32.2 0.082\n",
      "buy: 20190524-07-09 1836 0 14.7 0.11\n",
      "☆hit!☆: 20190524-07-09 0 14.7\n",
      "buy: 20190531-01-01 1837 0 16.5 0.095\n",
      "buy: 20190531-01-01 1837 1 19.2 0.081\n",
      "buy: 20190130-15-06 1841 0 26.6 0.093\n",
      "buy: 20190130-15-06 1841 4 30.4 0.081\n",
      "buy: 20190325-15-03 1857 0 50.1 0.136\n",
      "buy: 20190205-11-09 1865 0 39.1 0.092\n",
      "buy: 20190510-18-08 1867 0 18.6 0.084\n",
      "☆hit!☆: 20190510-18-08 0 18.6\n",
      "buy: 20190501-08-04 1870 0 37.1 0.089\n",
      "buy: 20190415-12-09 1881 0 13.0 0.136\n",
      "buy: 20190112-13-07 1890 0 54.7 0.173\n",
      "buy: 20190112-13-07 1890 1 29.4 0.118\n",
      "buy: 20190113-08-06 1891 0 36.0 0.115\n",
      "buy: 20190113-08-06 1891 1 54.8 0.086\n",
      "buy: 20190118-13-04 1892 0 31.4 0.14\n",
      "buy: 20190211-07-04 1898 0 108.8 0.109\n",
      "buy: 20190529-04-07 1904 0 30.9 0.184\n",
      "buy: 20190529-04-07 1904 1 82.2 0.083\n",
      "buy: 20190121-19-03 1921 0 28.9 0.135\n",
      "buy: 20190121-19-03 1921 1 35.9 0.102\n",
      "buy: 20190506-14-04 1925 0 40.0 0.098\n",
      "buy: 20190527-24-12 1931 0 15.6 0.101\n",
      "☆hit!☆: 20190527-24-12 0 15.6\n",
      "buy: 20190422-13-02 1933 0 78.3 0.156\n",
      "buy: 20190503-08-09 1942 0 15.2 0.106\n",
      "buy: 20190421-02-04 1948 0 50.2 0.091\n",
      "buy: 20190530-17-11 1952 1 24.8 0.083\n",
      "buy: 20190530-17-11 1952 4 26.6 0.081\n",
      "buy: 20190510-17-07 1954 0 50.5 0.12\n",
      "buy: 20190124-18-05 1961 0 14.1 0.112\n",
      "buy: 20190105-20-05 1969 0 15.9 0.097\n",
      "buy: 20190428-07-02 1977 0 25.8 0.082\n",
      "buy: 20190521-04-02 1981 0 35.8 0.099\n",
      "☆hit!☆: 20190521-04-02 0 35.8\n",
      "buy: 20190213-14-07 1985 0 22.1 0.093\n",
      "buy: 20190124-04-03 1989 0 61.4 0.143\n",
      "buy: 20190115-12-06 1997 0 134.5 0.092\n",
      "buy: 20190422-20-06 1999 0 28.5 0.095\n",
      "buy: 20190223-14-01 2006 0 19.8 0.081\n",
      "buy: 20190520-04-05 2012 0 63.0 0.084\n",
      "buy: 20190320-02-07 2015 0 20.1 0.092\n",
      "buy: 20190216-15-07 2016 0 35.2 0.164\n",
      "buy: 20190516-07-07 2019 0 30.9 0.096\n",
      "buy: 20190220-13-03 2020 0 33.4 0.102\n",
      "buy: 20190102-01-01 2021 0 83.9 0.095\n",
      "buy: 20190521-19-09 2023 0 30.7 0.096\n",
      "buy: 20190220-07-03 2025 0 28.0 0.13\n",
      "buy: 20190330-06-08 2032 0 100.8 0.081\n",
      "buy: 20190325-23-01 2034 0 51.6 0.095\n",
      "buy: 20190416-05-02 2035 0 43.4 0.107\n",
      "buy: 20190529-19-05 2037 0 23.7 0.15\n",
      "buy: 20190226-09-09 2039 0 12.1 0.124\n",
      "buy: 20190102-01-08 2042 1 22.5 0.099\n",
      "buy: 20190303-20-02 2048 0 76.1 0.101\n",
      "buy: 20190108-04-10 2052 1 15.5 0.123\n",
      "buy: 20190409-18-10 2062 0 53.4 0.086\n",
      "buy: 20190416-07-02 2064 0 14.0 0.109\n",
      "buy: 20190416-07-02 2064 1 18.8 0.089\n",
      "buy: 20190212-14-08 2077 0 44.8 0.134\n",
      "buy: 20190117-15-04 2090 0 28.1 0.122\n",
      "buy: 20190324-11-09 2096 0 37.6 0.121\n",
      "buy: 20190102-09-06 2102 0 16.8 0.1\n",
      "☆hit!☆: 20190102-09-06 0 16.8\n",
      "buy: 20190222-13-07 2110 1 158.5 0.083\n",
      "buy: 20190308-24-07 2111 1 45.1 0.084\n",
      "buy: 20190402-03-08 2112 0 21.0 0.114\n",
      "buy: 20190319-20-04 2123 0 17.1 0.103\n",
      "buy: 20190421-17-09 2129 0 16.7 0.113\n",
      "☆hit!☆: 20190421-17-09 0 16.7\n",
      "buy: 20190221-03-05 2132 0 59.3 0.084\n",
      "buy: 20190203-24-05 2134 0 14.1 0.113\n",
      "buy: 20190223-02-12 2139 0 24.4 0.097\n",
      "buy: 20190105-01-07 2152 0 13.5 0.183\n",
      "buy: 20190105-01-07 2152 1 40.9 0.097\n",
      "buy: 20190511-02-10 2155 0 20.9 0.091\n",
      "buy: 20190219-24-08 2172 0 46.6 0.082\n",
      "buy: 20190323-16-12 2175 0 15.3 0.129\n",
      "buy: 20190323-16-12 2175 1 16.9 0.101\n",
      "buy: 20190426-07-08 2177 0 39.6 0.158\n",
      "buy: 20190426-07-08 2177 1 135.7 0.089\n",
      "buy: 20190312-19-07 2178 0 12.7 0.144\n",
      "buy: 20190312-19-07 2178 1 51.5 0.087\n",
      "buy: 20190327-22-09 2186 0 68.2 0.097\n",
      "buy: 20190107-05-04 2197 0 36.7 0.083\n",
      "buy: 20190402-06-07 2204 0 27.5 0.092\n",
      "buy: 20190123-24-02 2209 0 51.2 0.097\n",
      "buy: 20190123-24-02 2209 1 35.6 0.103\n",
      "buy: 20190327-17-07 2217 0 30.5 0.095\n",
      "buy: 20190409-22-06 2219 0 77.7 0.085\n",
      "☆hit!☆: 20190409-22-06 0 77.7\n",
      "buy: 20190129-16-03 2229 0 33.8 0.095\n",
      "buy: 20190105-06-07 2230 0 146.2 0.089\n",
      "buy: 20190310-03-07 2234 0 19.2 0.083\n",
      "buy: 20190310-03-07 2234 1 41.2 0.087\n",
      "buy: 20190323-07-07 2250 0 56.7 0.092\n",
      "buy: 20190528-09-11 2258 0 20.7 0.098\n",
      "buy: 20190318-24-12 2259 0 17.4 0.095\n",
      "buy: 20190420-01-08 2260 0 20.0 0.109\n",
      "buy: 20190516-03-01 2282 0 32.0 0.128\n",
      "buy: 20190306-11-06 2289 0 27.5 0.101\n",
      "buy: 20190524-24-06 2294 0 18.8 0.163\n",
      "buy: 20190524-24-06 2294 1 47.8 0.083\n",
      "buy: 20190311-06-05 2298 0 10.6 0.21\n",
      "buy: 20190311-06-05 2298 1 30.4 0.096\n",
      "buy: 20190102-23-08 2304 0 18.6 0.096\n",
      "buy: 20190517-12-08 2321 0 46.2 0.097\n",
      "buy: 20190501-23-11 2326 0 22.3 0.08\n",
      "buy: 20190506-06-07 2335 0 14.1 0.107\n",
      "buy: 20190506-06-07 2335 1 29.6 0.08\n",
      "☆hit!☆: 20190506-06-07 1 29.6\n",
      "buy: 20190114-12-02 2337 0 22.6 0.081\n",
      "buy: 20190121-03-08 2339 0 29.4 0.088\n",
      "buy: 20190206-14-01 2352 0 12.8 0.171\n",
      "buy: 20190212-08-11 2361 0 23.3 0.099\n",
      "buy: 20190212-08-11 2361 5 55.5 0.081\n",
      "buy: 20190506-13-12 2364 0 28.7 0.082\n",
      "buy: 20190506-13-12 2364 1 26.6 0.083\n",
      "buy: 20190515-01-02 2366 0 14.1 0.163\n",
      "buy: 20190515-01-02 2366 4 33.4 0.088\n",
      "buy: 20190515-01-02 2366 5 32.1 0.093\n",
      "buy: 20190114-10-10 2367 0 43.2 0.117\n",
      "buy: 20190125-15-01 2385 0 26.9 0.086\n",
      "buy: 20190112-05-10 2390 0 18.8 0.099\n",
      "buy: 20190526-10-05 2404 0 22.8 0.084\n",
      "buy: 20190307-18-03 2405 0 39.7 0.097\n",
      "buy: 20190318-02-04 2408 0 75.1 0.09\n",
      "buy: 20190318-02-04 2408 1 76.6 0.088\n",
      "buy: 20190503-12-08 2430 0 77.3 0.11\n",
      "buy: 20190518-19-04 2431 0 21.6 0.098\n",
      "buy: 20190114-05-02 2437 0 52.6 0.088\n",
      "buy: 20190114-05-02 2437 1 114.4 0.088\n",
      "buy: 20190530-12-05 2443 0 16.5 0.092\n",
      "buy: 20190106-14-07 2444 0 55.3 0.085\n",
      "buy: 20190506-07-01 2445 0 20.3 0.142\n",
      "buy: 20190126-02-02 2446 0 52.1 0.115\n",
      "buy: 20190126-02-02 2446 1 48.5 0.095\n",
      "buy: 20190430-20-03 2447 0 52.3 0.155\n",
      "buy: 20190430-20-03 2447 1 80.5 0.106\n",
      "buy: 20190421-13-02 2448 0 25.3 0.134\n",
      "buy: 20190503-06-01 2451 0 24.0 0.117\n",
      "buy: 20190503-06-01 2451 1 58.1 0.086\n",
      "buy: 20190217-18-01 2453 0 15.1 0.141\n",
      "buy: 20190104-11-02 2454 0 55.5 0.156\n",
      "☆hit!☆: 20190104-11-02 0 55.5\n",
      "buy: 20190107-14-11 2456 0 21.0 0.094\n",
      "buy: 20190507-07-06 2458 0 21.8 0.094\n",
      "buy: 20190203-12-05 2467 0 45.0 0.127\n",
      "buy: 20190216-02-08 2471 0 46.7 0.087\n",
      "buy: 20190311-20-11 2482 0 16.5 0.108\n",
      "buy: 20190117-20-07 2485 0 66.4 0.125\n",
      "buy: 20190303-08-06 2495 1 62.4 0.086\n",
      "buy: 20190308-18-02 2496 0 35.2 0.103\n",
      "buy: 20190106-10-08 2503 0 28.1 0.099\n",
      "buy: 20190221-19-02 2511 0 22.3 0.103\n",
      "buy: 20190221-19-02 2511 1 19.1 0.088\n",
      "buy: 20190512-06-02 2514 0 42.9 0.084\n",
      "buy: 20190527-20-11 2515 0 23.1 0.084\n",
      "☆hit!☆: 20190527-20-11 0 23.1\n",
      "buy: 20190317-15-07 2519 0 29.2 0.106\n",
      "buy: 20190122-04-03 2523 0 61.9 0.107\n",
      "buy: 20190122-04-03 2523 1 60.4 0.096\n",
      "buy: 20190525-07-07 2527 0 16.3 0.13\n",
      "buy: 20190207-01-03 2532 0 17.6 0.093\n",
      "buy: 20190423-09-04 2541 0 26.8 0.082\n",
      "buy: 20190326-13-11 2547 0 27.4 0.143\n",
      "buy: 20190217-04-07 2549 0 33.3 0.085\n",
      "buy: 20190119-16-01 2553 0 18.5 0.111\n",
      "buy: 20190506-14-09 2560 0 26.2 0.088\n",
      "buy: 20190214-01-03 2569 0 17.1 0.095\n",
      "☆hit!☆: 20190214-01-03 0 17.1\n",
      "buy: 20190322-05-09 2575 0 23.4 0.087\n",
      "buy: 20190301-06-08 2576 0 19.5 0.106\n",
      "buy: 20190301-06-08 2576 1 67.6 0.084\n",
      "buy: 20190506-05-04 2578 0 43.2 0.091\n",
      "buy: 20190202-02-07 2580 1 18.3 0.09\n",
      "buy: 20190511-24-06 2581 0 19.0 0.083\n",
      "buy: 20190316-13-06 2584 0 16.0 0.101\n",
      "buy: 20190426-05-01 2585 0 13.8 0.124\n",
      "☆hit!☆: 20190426-05-01 0 13.8\n",
      "buy: 20190123-11-08 2588 1 24.0 0.091\n",
      "buy: 20190510-19-09 2589 0 34.7 0.082\n",
      "buy: 20190201-16-02 2598 0 19.4 0.095\n",
      "buy: 20190114-22-02 2608 0 34.9 0.138\n",
      "buy: 20190114-22-02 2608 1 164.5 0.103\n",
      "buy: 20190114-22-02 2608 5 228.6 0.084\n",
      "buy: 20190404-21-05 2612 0 54.2 0.089\n",
      "buy: 20190223-17-02 2613 0 22.3 0.093\n",
      "buy: 20190207-06-06 2616 0 56.2 0.082\n",
      "buy: 20190315-13-05 2618 0 20.8 0.125\n",
      "buy: 20190315-13-05 2618 1 63.3 0.12\n",
      "buy: 20190330-03-07 2620 0 26.7 0.104\n",
      "buy: 20190512-05-02 2623 0 33.3 0.105\n",
      "buy: 20190512-05-02 2623 5 97.1 0.093\n",
      "buy: 20190505-16-05 2634 0 29.1 0.091\n",
      "buy: 20190501-02-01 2641 1 35.0 0.085\n",
      "buy: 20190223-24-02 2645 1 22.5 0.118\n",
      "buy: 20190418-05-10 2649 0 64.5 0.083\n",
      "buy: 20190120-15-04 2655 0 26.4 0.09\n",
      "buy: 20190217-04-06 2664 0 20.9 0.111\n",
      "buy: 20190322-11-02 2676 0 25.3 0.095\n",
      "buy: 20190114-12-06 2678 0 29.3 0.09\n",
      "buy: 20190116-17-03 2680 0 15.0 0.133\n",
      "buy: 20190116-17-03 2680 1 38.2 0.089\n",
      "buy: 20190427-05-03 2682 0 32.9 0.083\n",
      "buy: 20190311-19-06 2685 1 41.7 0.09\n",
      "buy: 20190311-19-06 2685 5 79.9 0.088\n",
      "buy: 20190302-22-11 2686 0 9.7 0.163\n",
      "buy: 20190519-19-03 2687 0 28.4 0.082\n",
      "buy: 20190321-05-08 2695 0 48.0 0.103\n",
      "buy: 20190122-13-09 2698 0 18.7 0.116\n",
      "buy: 20190418-11-04 2705 0 25.5 0.082\n",
      "buy: 20190513-05-10 2716 0 64.0 0.112\n",
      "buy: 20190125-14-09 2718 0 26.1 0.115\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy: 20190330-04-07 2728 0 58.8 0.104\n",
      "buy: 20190413-20-04 2731 1 34.0 0.094\n",
      "buy: 20190421-10-07 2733 0 24.3 0.115\n",
      "buy: 20190105-05-07 2734 0 53.7 0.082\n",
      "buy: 20190106-19-07 2735 0 20.4 0.09\n",
      "buy: 20190505-19-04 2736 0 12.4 0.123\n",
      "☆hit!☆: 20190505-19-04 0 12.4\n",
      "buy: 20190302-07-11 2739 0 18.6 0.086\n",
      "buy: 20190504-21-02 2752 0 89.2 0.137\n",
      "☆hit!☆: 20190504-21-02 0 89.2\n",
      "buy: 20190504-21-02 2752 1 38.1 0.11\n",
      "buy: 20190315-24-01 2753 0 18.9 0.119\n",
      "buy: 20190206-15-03 2763 0 78.6 0.086\n",
      "buy: 20190417-23-09 2768 0 22.4 0.094\n",
      "buy: 20190320-15-04 2779 0 33.7 0.164\n",
      "buy: 20190320-15-04 2779 1 91.8 0.084\n",
      "buy: 20190414-12-06 2780 0 16.9 0.129\n",
      "buy: 20190320-01-11 2781 1 21.9 0.091\n",
      "buy: 20190422-02-09 2787 0 16.4 0.114\n",
      "buy: 20190422-02-09 2787 1 35.8 0.099\n",
      "buy: 20190221-03-08 2790 0 35.4 0.098\n",
      "☆hit!☆: 20190221-03-08 0 35.4\n",
      "buy: 20190117-15-02 2793 0 34.4 0.102\n",
      "buy: 20190326-14-06 2795 0 29.2 0.187\n",
      "☆hit!☆: 20190326-14-06 0 29.2\n",
      "buy: 20190224-13-03 2805 0 28.9 0.081\n",
      "buy: 20190125-15-03 2811 0 50.7 0.101\n",
      "buy: 20190313-13-04 2823 0 38.1 0.086\n",
      "buy: 20190313-13-04 2823 1 40.7 0.083\n",
      "buy: 20190502-22-05 2824 0 27.9 0.081\n",
      "buy: 20190502-22-05 2824 1 123.2 0.13\n",
      "buy: 20190222-13-04 2826 0 12.4 0.161\n",
      "buy: 20190308-15-12 2828 0 26.4 0.099\n",
      "buy: 20190215-01-01 2830 0 29.4 0.125\n",
      "buy: 20190517-08-04 2835 0 52.0 0.101\n",
      "buy: 20190517-08-04 2835 1 50.6 0.093\n",
      "buy: 20190224-01-05 2837 0 165.5 0.083\n",
      "buy: 20190408-15-04 2840 0 25.6 0.112\n",
      "buy: 20190408-15-04 2840 1 38.4 0.102\n",
      "buy: 20190312-04-12 2844 0 17.6 0.093\n",
      "buy: 20190312-04-12 2844 1 29.8 0.087\n",
      "buy: 20190121-18-07 2848 0 24.4 0.085\n",
      "buy: 20190409-02-01 2852 0 15.8 0.096\n",
      "buy: 20190409-02-01 2852 1 57.5 0.086\n",
      "buy: 20190116-01-05 2854 0 24.9 0.08\n",
      "buy: 20190405-13-04 2857 0 64.2 0.118\n",
      "buy: 20190216-01-08 2872 1 36.0 0.09\n",
      "buy: 20190303-21-01 2876 0 17.1 0.098\n",
      "buy: 20190419-18-11 2877 1 31.0 0.104\n",
      "buy: 20190106-20-04 2878 0 54.1 0.133\n",
      "☆hit!☆: 20190106-20-04 0 54.1\n",
      "buy: 20190106-20-04 2878 1 56.7 0.089\n",
      "buy: 20190429-16-07 2884 0 21.6 0.112\n",
      "☆hit!☆: 20190429-16-07 0 21.6\n",
      "buy: 20190429-16-07 2884 1 16.0 0.102\n",
      "buy: 20190502-20-03 2885 0 66.2 0.125\n",
      "buy: 20190129-14-06 2889 0 39.4 0.138\n",
      "buy: 20190129-14-06 2889 1 45.2 0.091\n",
      "buy: 20190220-07-06 2893 0 20.7 0.111\n",
      "buy: 20190331-02-06 2895 0 22.6 0.082\n",
      "buy: 20190212-07-01 2897 0 49.6 0.094\n",
      "buy: 20190323-23-05 2901 0 18.3 0.099\n",
      "buy: 20190422-06-06 2911 0 50.4 0.114\n",
      "buy: 20190422-06-06 2911 1 173.6 0.083\n",
      "buy: 20190307-01-01 2929 0 49.8 0.087\n",
      "buy: 20190503-10-03 2946 0 13.4 0.177\n",
      "buy: 20190503-10-03 2946 1 55.8 0.092\n",
      "buy: 20190123-06-11 2949 0 19.0 0.086\n",
      "buy: 20190503-02-09 2950 0 91.3 0.08\n",
      "buy: 20190311-19-03 2955 0 36.2 0.083\n",
      "buy: 20190112-20-07 2958 0 18.1 0.101\n",
      "buy: 20190521-01-01 2965 0 40.2 0.118\n",
      "buy: 20190125-18-05 2967 0 41.8 0.084\n",
      "buy: 20190116-04-08 2969 0 19.6 0.117\n",
      "buy: 20190116-04-08 2969 1 46.0 0.089\n",
      "buy: 20190331-22-04 2972 0 26.7 0.113\n",
      "buy: 20190410-16-07 2975 0 26.9 0.08\n",
      "buy: 20190310-12-08 2980 0 14.5 0.104\n",
      "buy: 20190516-07-05 2985 0 34.0 0.093\n",
      "buy: 20190107-10-02 2997 0 14.0 0.169\n",
      "buy: 20190323-16-03 3004 0 34.9 0.179\n",
      "buy: 20190323-16-03 3004 1 52.3 0.088\n",
      "buy: 20190323-16-03 3004 4 29.2 0.098\n",
      "☆hit!☆: 20190323-16-03 4 29.2\n",
      "buy: 20190120-16-05 3013 0 36.8 0.097\n",
      "buy: 20190120-16-05 3013 1 39.6 0.088\n",
      "buy: 20190429-24-02 3017 0 25.8 0.101\n",
      "buy: 20190424-23-12 3021 0 14.7 0.172\n",
      "buy: 20190131-24-03 3022 0 36.6 0.111\n",
      "buy: 20190418-16-07 3025 0 82.5 0.091\n",
      "buy: 20190418-16-07 3025 1 61.4 0.116\n",
      "☆hit!☆: 20190418-16-07 1 61.4\n",
      "buy: 20190530-02-08 3028 0 38.2 0.12\n",
      "buy: 20190530-02-08 3028 1 20.8 0.089\n",
      "buy: 20190413-06-08 3038 0 22.8 0.141\n",
      "buy: 20190413-06-08 3038 1 60.9 0.106\n",
      "buy: 20190413-06-08 3038 5 83.9 0.083\n",
      "buy: 20190123-17-10 3045 0 120.0 0.109\n",
      "buy: 20190123-17-10 3045 1 70.4 0.083\n",
      "☆hit!☆: 20190123-17-10 1 70.4\n",
      "buy: 20190506-16-05 3057 0 45.3 0.111\n",
      "buy: 20190506-16-05 3057 1 79.0 0.094\n",
      "buy: 20190105-10-06 3058 0 34.8 0.092\n",
      "buy: 20190322-15-04 3060 0 35.9 0.121\n",
      "buy: 20190217-01-09 3063 0 21.5 0.086\n",
      "buy: 20190314-17-05 3065 1 15.7 0.152\n",
      "buy: 20190123-17-04 3066 0 38.8 0.084\n",
      "buy: 20190324-10-08 3091 0 20.4 0.109\n",
      "buy: 20190117-14-06 3100 0 64.3 0.08\n",
      "buy: 20190112-11-11 3102 0 18.1 0.117\n",
      "buy: 20190109-14-08 3103 0 21.6 0.086\n",
      "buy: 20190127-15-10 3104 0 38.8 0.104\n",
      "buy: 20190127-15-10 3104 1 21.4 0.088\n",
      "buy: 20190222-13-05 3110 0 31.9 0.09\n",
      "☆hit!☆: 20190222-13-05 0 31.9\n",
      "buy: 20190116-10-07 3118 0 41.8 0.082\n",
      "buy: 20190203-10-08 3120 0 50.8 0.098\n",
      "buy: 20190323-21-03 3131 0 17.1 0.14\n",
      "buy: 20190219-13-04 3133 0 18.5 0.09\n",
      "☆hit!☆: 20190219-13-04 0 18.5\n",
      "buy: 20190309-09-04 3134 0 45.6 0.166\n",
      "buy: 20190123-18-08 3135 0 15.1 0.149\n",
      "buy: 20190123-18-08 3135 1 19.4 0.117\n",
      "buy: 20190505-22-06 3140 0 17.3 0.115\n",
      "buy: 20190129-09-06 3146 0 23.6 0.104\n",
      "buy: 20190331-08-05 3150 0 34.2 0.082\n",
      "buy: 20190105-06-02 3156 0 52.6 0.081\n",
      "buy: 20190103-11-09 3158 0 49.6 0.09\n",
      "buy: 20190403-21-09 3162 0 24.2 0.143\n",
      "buy: 20190113-10-06 3164 0 39.0 0.09\n",
      "buy: 20190112-03-08 3170 0 19.3 0.096\n",
      "buy: 20190227-15-02 3176 0 24.4 0.102\n",
      "buy: 20190326-18-08 3181 0 23.1 0.102\n",
      "buy: 20190110-15-03 3186 0 54.5 0.082\n",
      "buy: 20190422-06-02 3189 0 18.9 0.081\n",
      "buy: 20190212-14-09 3193 0 17.2 0.141\n",
      "☆hit!☆: 20190212-14-09 0 17.2\n",
      "buy: 20190329-03-06 3202 0 13.5 0.114\n",
      "buy: 20190301-06-05 3205 0 41.9 0.118\n",
      "☆hit!☆: 20190301-06-05 0 41.9\n",
      "buy: 20190301-06-05 3205 1 66.0 0.097\n",
      "buy: 20190302-13-04 3219 0 181.3 0.094\n",
      "buy: 20190302-13-04 3219 1 128.3 0.082\n",
      "buy: 20190326-22-09 3222 0 45.7 0.11\n",
      "buy: 20190112-02-01 3234 0 17.0 0.089\n",
      "buy: 20190503-01-03 3244 0 32.8 0.116\n",
      "buy: 20190417-12-04 3251 0 57.6 0.148\n",
      "buy: 20190331-15-04 3254 0 24.9 0.107\n",
      "buy: 20190212-14-11 3262 0 24.6 0.104\n",
      "buy: 20190109-24-02 3264 0 18.2 0.118\n",
      "buy: 20190521-04-10 3266 0 37.9 0.141\n",
      "buy: 20190316-14-02 3271 0 103.0 0.107\n",
      "buy: 20190512-01-01 3283 0 68.6 0.084\n",
      "buy: 20190530-16-12 3285 0 16.5 0.105\n",
      "buy: 20190117-12-07 3286 0 45.1 0.084\n",
      "buy: 20190224-16-10 3290 4 19.5 0.08\n",
      "buy: 20190319-16-04 3291 0 14.5 0.122\n",
      "buy: 20190314-01-03 3293 0 30.9 0.102\n",
      "buy: 20190525-07-04 3296 0 19.9 0.08\n",
      "buy: 20190518-02-04 3316 0 59.3 0.107\n",
      "buy: 20190520-20-01 3341 5 24.5 0.085\n",
      "buy: 20190512-19-06 3347 0 22.3 0.08\n",
      "buy: 20190504-03-03 3348 0 15.6 0.151\n",
      "buy: 20190504-03-03 3348 1 31.7 0.103\n",
      "buy: 20190317-17-08 3354 0 24.4 0.088\n",
      "buy: 20190407-03-06 3364 1 41.1 0.087\n",
      "buy: 20190203-05-05 3368 0 16.2 0.093\n",
      "buy: 20190203-05-05 3368 4 34.8 0.09\n",
      "buy: 20190105-11-06 3371 0 23.8 0.101\n",
      "buy: 20190517-20-03 3385 0 29.6 0.086\n",
      "buy: 20190210-15-02 3390 0 23.4 0.103\n",
      "buy: 20190323-10-05 3393 0 32.6 0.093\n",
      "☆hit!☆: 20190323-10-05 0 32.6\n",
      "buy: 20190223-24-06 3406 0 22.4 0.116\n",
      "buy: 20190504-12-05 3408 0 11.9 0.138\n",
      "buy: 20190504-12-05 3408 1 44.0 0.085\n",
      "☆hit!☆: 20190504-12-05 1 44.0\n",
      "buy: 20190504-12-05 3408 5 53.1 0.082\n",
      "buy: 20190110-06-12 3413 0 18.2 0.12\n",
      "buy: 20190110-06-12 3413 4 19.5 0.084\n",
      "buy: 20190417-19-07 3429 0 74.7 0.117\n",
      "buy: 20190120-03-06 3439 0 41.7 0.129\n",
      "buy: 20190311-19-01 3453 0 29.5 0.082\n",
      "buy: 20190225-02-01 3477 0 33.9 0.089\n",
      "buy: 20190509-21-07 3483 0 16.3 0.113\n",
      "buy: 20190319-18-11 3499 0 18.0 0.086\n",
      "buy: 20190308-14-07 3500 0 49.2 0.142\n",
      "buy: 20190501-13-02 3503 0 83.3 0.084\n",
      "buy: 20190123-23-07 3521 0 19.9 0.082\n",
      "buy: 20190218-17-01 3525 0 21.4 0.138\n",
      "buy: 20190218-17-01 3525 1 30.2 0.129\n",
      "buy: 20190511-09-04 3530 0 21.5 0.129\n",
      "☆hit!☆: 20190511-09-04 0 21.5\n",
      "buy: 20190428-15-11 3542 1 47.7 0.085\n",
      "buy: 20190421-04-10 3551 0 16.5 0.092\n",
      "buy: 20190511-02-01 3560 0 16.9 0.106\n",
      "buy: 20190502-11-03 3565 0 15.6 0.107\n",
      "☆hit!☆: 20190502-11-03 0 15.6\n",
      "buy: 20190502-11-03 3565 1 26.9 0.083\n",
      "buy: 20190119-09-04 3578 0 47.2 0.095\n",
      "buy: 20190112-04-07 3580 0 33.2 0.088\n",
      "buy: 20190130-03-04 3590 0 20.4 0.211\n",
      "buy: 20190130-03-04 3590 4 21.9 0.11\n",
      "buy: 20190209-07-05 3591 0 26.7 0.103\n",
      "buy: 20190209-15-06 3592 0 59.2 0.085\n",
      "buy: 20190303-10-12 3594 0 20.6 0.121\n",
      "☆hit!☆: 20190303-10-12 0 20.6\n",
      "buy: 20190303-10-12 3594 1 64.2 0.082\n",
      "buy: 20190426-17-03 3600 0 34.7 0.114\n",
      "buy: 20190312-07-07 3605 0 42.7 0.153\n",
      "buy: 20190101-14-06 3622 0 108.0 0.115\n",
      "buy: 20190312-10-01 3630 0 35.0 0.087\n",
      "buy: 20190205-05-02 3636 0 25.2 0.146\n",
      "buy: 20190205-05-02 3636 1 19.5 0.092\n",
      "buy: 20190527-21-08 3637 0 27.8 0.095\n",
      "buy: 20190416-09-06 3639 0 34.6 0.086\n",
      "buy: 20190111-20-07 3641 0 57.1 0.1\n",
      "buy: 20190423-09-08 3647 0 56.3 0.088\n",
      "buy: 20190428-07-06 3649 0 70.4 0.104\n",
      "buy: 20190428-07-06 3649 1 18.3 0.109\n",
      "buy: 20190518-08-02 3650 0 30.3 0.085\n",
      "buy: 20190518-08-02 3650 1 65.4 0.112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy: 20190512-01-02 3658 0 39.7 0.083\n",
      "buy: 20190407-23-10 3669 1 29.3 0.088\n",
      "buy: 20190123-04-10 3681 0 14.3 0.115\n",
      "buy: 20190130-01-02 3683 0 21.2 0.081\n",
      "buy: 20190418-17-02 3687 0 25.1 0.092\n",
      "buy: 20190313-03-02 3692 0 20.5 0.088\n",
      "☆hit!☆: 20190313-03-02 0 20.5\n",
      "buy: 20190320-01-06 3693 0 17.4 0.122\n",
      "buy: 20190207-15-03 3695 0 181.0 0.092\n",
      "buy: 20190422-07-03 3709 0 72.3 0.095\n",
      "buy: 20190422-07-03 3709 1 98.6 0.082\n",
      "buy: 20190116-11-07 3717 0 97.5 0.106\n",
      "buy: 20190412-07-05 3722 0 56.5 0.122\n",
      "buy: 20190504-04-02 3738 0 54.3 0.084\n",
      "buy: 20190117-21-05 3741 0 26.3 0.126\n",
      "buy: 20190318-18-06 3751 0 18.3 0.085\n",
      "buy: 20190122-23-08 3752 1 23.3 0.082\n",
      "buy: 20190118-01-01 3759 0 21.5 0.08\n",
      "buy: 20190319-01-08 3772 0 23.1 0.124\n",
      "buy: 20190409-23-08 3778 0 28.9 0.093\n",
      "buy: 20190409-23-08 3778 4 39.3 0.082\n",
      "buy: 20190110-23-05 3779 0 38.1 0.09\n",
      "☆hit!☆: 20190110-23-05 0 38.1\n",
      "buy: 20190202-10-06 3781 0 27.3 0.091\n",
      "buy: 20190305-12-02 3782 0 35.6 0.092\n",
      "buy: 20190305-12-02 3782 1 44.3 0.08\n",
      "buy: 20190407-05-07 3787 0 18.6 0.117\n",
      "buy: 20190407-05-07 3787 1 41.5 0.087\n",
      "buy: 20190121-18-11 3790 0 21.7 0.083\n",
      "buy: 20190521-20-12 3795 0 44.2 0.094\n",
      "buy: 20190524-22-03 3804 0 59.6 0.113\n",
      "☆hit!☆: 20190524-22-03 0 59.6\n",
      "buy: 20190311-04-02 3805 0 29.3 0.152\n",
      "buy: 20190425-12-06 3809 1 46.8 0.09\n",
      "buy: 20190524-10-07 3814 0 92.0 0.084\n",
      "buy: 20190524-10-07 3814 5 322.9 0.097\n",
      "buy: 20190501-14-12 3818 0 19.6 0.082\n",
      "☆hit!☆: 20190501-14-12 0 19.6\n",
      "buy: 20190509-18-11 3819 0 22.0 0.136\n",
      "buy: 20190205-07-05 3820 1 39.0 0.095\n",
      "buy: 20190302-12-12 3834 0 24.7 0.098\n",
      "buy: 20190320-15-09 3839 1 40.7 0.088\n",
      "buy: 20190206-17-10 3848 0 25.4 0.101\n",
      "buy: 20190330-13-10 3850 0 14.4 0.172\n",
      "buy: 20190223-13-03 3852 0 13.7 0.146\n",
      "buy: 20190415-23-05 3857 0 45.4 0.105\n",
      "buy: 20190415-23-05 3857 1 50.2 0.087\n",
      "buy: 20190225-01-05 3859 0 65.9 0.093\n",
      "buy: 20190225-01-05 3859 1 16.7 0.09\n",
      "buy: 20190120-04-07 3868 0 54.4 0.093\n",
      "buy: 20190117-22-06 3870 1 19.6 0.153\n",
      "buy: 20190413-06-09 3876 4 20.2 0.082\n",
      "buy: 20190305-14-10 3884 5 39.4 0.097\n",
      "buy: 20190104-21-09 3888 0 59.8 0.119\n",
      "buy: 20190104-21-09 3888 4 77.1 0.082\n",
      "buy: 20190423-15-07 3891 0 32.8 0.152\n",
      "buy: 20190423-15-07 3891 1 16.2 0.095\n",
      "buy: 20190423-15-07 3891 4 59.8 0.1\n",
      "buy: 20190423-15-07 3891 5 68.1 0.09\n",
      "☆hit!☆: 20190423-15-07 5 68.1\n",
      "buy: 20190321-08-08 3896 0 43.6 0.1\n",
      "buy: 20190226-11-09 3897 1 24.1 0.085\n",
      "buy: 20190224-23-09 3908 0 31.7 0.083\n",
      "buy: 20190204-02-03 3913 0 26.8 0.099\n",
      "buy: 20190427-05-01 3922 0 31.3 0.085\n",
      "buy: 20190514-03-06 3923 1 68.3 0.084\n",
      "buy: 20190502-19-08 3940 0 41.5 0.08\n",
      "buy: 20190502-19-08 3940 1 28.7 0.084\n",
      "buy: 20190405-19-05 3945 0 15.9 0.095\n",
      "☆hit!☆: 20190405-19-05 0 15.9\n",
      "buy: 20190513-02-07 3946 0 33.8 0.16\n",
      "buy: 20190513-02-07 3946 1 26.8 0.123\n",
      "buy: 20190207-06-09 3953 0 16.1 0.112\n",
      "buy: 20190326-04-04 3960 1 42.2 0.085\n",
      "buy: 20190423-15-06 3966 0 15.1 0.117\n",
      "buy: 20190122-09-09 3976 0 59.2 0.083\n",
      "buy: 20190104-03-09 3977 0 27.0 0.092\n",
      "buy: 20190301-08-06 3996 0 30.2 0.096\n",
      "buy: 20190509-14-03 4003 0 14.2 0.114\n",
      "buy: 20190509-14-03 4003 1 34.3 0.098\n",
      "buy: 20190422-23-05 4007 1 54.9 0.084\n",
      "buy: 20190216-15-09 4008 0 5.8 0.271\n",
      "buy: 20190216-15-09 4008 1 17.0 0.099\n",
      "buy: 20190330-04-06 4012 0 71.6 0.096\n",
      "buy: 20190311-12-06 4015 0 18.6 0.103\n",
      "buy: 20190402-06-03 4021 0 114.4 0.105\n",
      "buy: 20190402-06-03 4021 1 272.3 0.08\n",
      "buy: 20190307-01-02 4024 0 92.5 0.08\n",
      "buy: 20190220-12-10 4026 0 17.7 0.106\n",
      "buy: 20190131-07-03 4034 0 55.6 0.112\n",
      "buy: 20190131-07-03 4034 1 27.5 0.081\n",
      "buy: 20190301-12-02 4042 0 22.1 0.127\n",
      "buy: 20190324-06-11 4043 0 13.0 0.158\n",
      "buy: 20190324-06-11 4043 1 23.8 0.101\n",
      "buy: 20190506-13-02 4046 0 91.1 0.083\n",
      "buy: 20190302-07-04 4048 0 17.8 0.089\n",
      "buy: 20190510-02-04 4050 0 42.4 0.124\n",
      "buy: 20190126-24-07 4052 0 71.8 0.11\n",
      "buy: 20190126-24-07 4052 1 120.8 0.081\n",
      "buy: 20190105-04-06 4057 0 31.7 0.085\n",
      "buy: 20190113-13-01 4058 0 27.7 0.087\n",
      "buy: 20190502-02-08 4059 0 14.4 0.132\n",
      "buy: 20190502-02-08 4059 5 113.8 0.087\n",
      "buy: 20190303-05-11 4060 1 28.6 0.083\n",
      "buy: 20190103-22-03 4061 0 23.3 0.106\n",
      "buy: 20190429-05-01 4070 0 57.4 0.085\n",
      "buy: 20190524-05-01 4072 0 38.1 0.128\n",
      "buy: 20190512-05-03 4083 1 66.2 0.087\n",
      "buy: 20190329-17-05 4100 0 17.9 0.103\n",
      "☆hit!☆: 20190329-17-05 0 17.9\n",
      "buy: 20190122-16-10 4101 0 27.0 0.09\n",
      "buy: 20190106-08-05 4108 0 23.1 0.087\n",
      "buy: 20190106-08-05 4108 1 84.2 0.11\n",
      "buy: 20190106-08-05 4108 5 21.3 0.086\n",
      "buy: 20190315-03-05 4113 0 17.0 0.101\n",
      "buy: 20190315-03-05 4113 5 17.8 0.114\n",
      "buy: 20190317-18-10 4118 0 12.8 0.119\n",
      "buy: 20190208-06-09 4119 0 18.9 0.119\n",
      "buy: 20190521-04-06 4123 0 56.5 0.105\n",
      "buy: 20190504-22-06 4126 0 16.7 0.102\n",
      "☆hit!☆: 20190504-22-06 0 16.7\n",
      "buy: 20190504-22-06 4126 1 62.0 0.082\n",
      "buy: 20190310-18-08 4129 0 18.9 0.09\n",
      "buy: 20190512-09-03 4133 0 13.9 0.159\n",
      "buy: 20190512-09-03 4133 1 40.5 0.113\n",
      "buy: 20190512-09-03 4133 4 17.5 0.089\n",
      "buy: 20190528-20-05 4136 5 88.1 0.086\n",
      "buy: 20190529-08-08 4148 0 17.2 0.102\n",
      "buy: 20190511-19-02 4151 0 17.7 0.128\n",
      "☆hit!☆: 20190511-19-02 0 17.7\n",
      "buy: 20190301-06-07 4156 0 51.3 0.082\n",
      "buy: 20190126-15-01 4164 0 44.6 0.137\n",
      "☆hit!☆: 20190126-15-01 0 44.6\n",
      "buy: 20190311-06-06 4165 0 121.3 0.154\n",
      "buy: 20190311-06-06 4165 1 42.7 0.104\n",
      "buy: 20190522-16-04 4167 0 95.9 0.13\n",
      "buy: 20190302-05-01 4185 0 20.9 0.114\n",
      "buy: 20190316-15-04 4186 0 10.8 0.21\n",
      "buy: 20190531-02-09 4187 0 55.0 0.104\n",
      "buy: 20190205-05-01 4188 0 52.8 0.106\n",
      "buy: 20190205-05-01 4188 1 17.5 0.107\n",
      "buy: 20190205-05-01 4188 4 76.3 0.082\n",
      "☆hit!☆: 20190205-05-01 4 76.3\n",
      "buy: 20190408-22-05 4192 0 16.6 0.099\n",
      "buy: 20190525-01-12 4198 0 16.7 0.112\n",
      "buy: 20190513-02-05 4204 0 37.2 0.099\n",
      "buy: 20190505-01-03 4211 0 16.3 0.151\n",
      "buy: 20190505-01-03 4211 1 32.1 0.082\n",
      "buy: 20190115-22-07 4212 0 64.4 0.094\n",
      "buy: 20190115-22-07 4212 1 52.2 0.081\n",
      "buy: 20190204-24-05 4221 0 22.8 0.083\n",
      "buy: 20190527-05-10 4228 1 29.8 0.091\n",
      "☆hit!☆: 20190527-05-10 1 29.8\n",
      "buy: 20190316-20-02 4231 0 39.4 0.103\n",
      "buy: 20190325-16-01 4239 0 13.3 0.146\n",
      "buy: 20190429-15-03 4240 0 26.0 0.098\n",
      "buy: 20190526-07-02 4245 0 15.5 0.115\n",
      "buy: 20190116-12-12 4250 0 12.4 0.147\n",
      "buy: 20190116-12-12 4250 1 40.0 0.098\n",
      "buy: 20190118-21-09 4252 0 15.3 0.118\n",
      "buy: 20190304-01-02 4255 0 47.3 0.086\n",
      "buy: 20190213-21-05 4265 0 14.9 0.122\n",
      "☆hit!☆: 20190213-21-05 0 14.9\n",
      "buy: 20190510-19-10 4266 0 15.0 0.116\n",
      "buy: 20190225-02-10 4267 0 57.3 0.108\n",
      "buy: 20190222-21-08 4268 0 18.8 0.11\n",
      "buy: 20190418-05-04 4274 0 23.0 0.116\n",
      "buy: 20190503-04-04 4281 0 112.8 0.087\n",
      "buy: 20190503-04-04 4281 1 149.9 0.093\n",
      "buy: 20190206-14-06 4285 0 19.6 0.116\n",
      "☆hit!☆: 20190206-14-06 0 19.6\n",
      "buy: 20190206-14-06 4285 1 73.0 0.089\n",
      "buy: 20190520-04-04 4294 0 39.6 0.081\n",
      "buy: 20190218-19-07 4300 4 21.7 0.084\n",
      "buy: 20190220-12-04 4302 0 20.4 0.119\n",
      "☆hit!☆: 20190220-12-04 0 20.4\n",
      "buy: 20190221-19-01 4304 0 17.5 0.11\n",
      "buy: 20190221-19-01 4304 4 26.2 0.098\n",
      "buy: 20190502-18-05 4308 0 28.4 0.095\n",
      "buy: 20190208-17-05 4314 0 26.8 0.101\n",
      "buy: 20190508-04-05 4318 0 56.7 0.105\n",
      "buy: 20190114-05-08 4322 0 102.7 0.089\n",
      "buy: 20190109-03-06 4326 0 46.1 0.085\n",
      "buy: 20190303-11-11 4332 1 22.6 0.092\n",
      "buy: 20190427-18-11 4340 0 43.5 0.119\n",
      "buy: 20190506-23-03 4345 0 20.9 0.123\n",
      "buy: 20190210-21-10 4347 0 19.7 0.136\n",
      "buy: 20190323-06-05 4357 0 115.1 0.096\n",
      "buy: 20190509-14-07 4369 1 72.7 0.087\n",
      "buy: 20190119-14-04 4374 0 33.9 0.104\n",
      "☆hit!☆: 20190119-14-04 0 33.9\n",
      "buy: 20190219-06-04 4380 0 30.7 0.104\n",
      "buy: 20190201-07-08 4386 0 63.7 0.086\n",
      "buy: 20190519-04-04 4388 0 57.4 0.13\n",
      "buy: 20190213-14-05 4398 0 19.6 0.082\n",
      "buy: 20190316-14-08 4406 0 26.6 0.087\n",
      "☆hit!☆: 20190316-14-08 0 26.6\n",
      "buy: 20190317-15-02 4410 0 26.0 0.196\n",
      "buy: 20190317-15-02 4410 1 25.9 0.129\n",
      "buy: 20190509-07-04 4414 0 30.9 0.139\n",
      "buy: 20190509-07-04 4414 1 25.1 0.107\n",
      "☆hit!☆: 20190509-07-04 1 25.1\n",
      "buy: 20190113-12-02 4421 1 18.5 0.097\n",
      "buy: 20190331-06-01 4426 0 29.1 0.087\n",
      "buy: 20190417-10-07 4445 0 26.8 0.126\n",
      "buy: 20190409-01-04 4454 0 91.2 0.096\n",
      "buy: 20190409-01-04 4454 1 226.0 0.083\n",
      "buy: 20190428-17-08 4455 0 51.6 0.092\n",
      "buy: 20190102-15-02 4456 0 48.2 0.11\n",
      "buy: 20190509-05-10 4467 0 16.6 0.103\n",
      "buy: 20190320-13-12 4474 0 20.6 0.102\n",
      "buy: 20190423-13-02 4477 0 23.4 0.086\n",
      "buy: 20190113-22-10 4484 1 19.0 0.085\n",
      "buy: 20190210-02-03 4490 0 22.5 0.095\n",
      "buy: 20190103-23-09 4492 0 113.0 0.088\n",
      "buy: 20190426-06-11 4497 0 18.3 0.092\n",
      "buy: 20190514-05-05 4511 0 23.2 0.103\n",
      "buy: 20190125-11-03 4515 0 37.7 0.106\n",
      "buy: 20190523-20-06 4521 0 39.5 0.084\n",
      "☆hit!☆: 20190523-20-06 0 39.5\n",
      "buy: 20190405-10-05 4522 0 42.7 0.123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy: 20190112-05-01 4538 0 16.6 0.112\n",
      "buy: 20190512-22-10 4544 0 12.1 0.13\n",
      "buy: 20190512-22-10 4544 1 18.8 0.101\n",
      "buy: 20190112-02-11 4545 0 28.2 0.11\n",
      "buy: 20190304-14-12 4548 0 8.8 0.214\n",
      "buy: 20190103-02-05 4550 0 143.8 0.098\n",
      "buy: 20190405-19-11 4556 0 26.1 0.122\n",
      "buy: 20190405-19-11 4556 1 15.2 0.1\n",
      "buy: 20190522-22-02 4563 0 22.2 0.107\n",
      "buy: 20190224-21-07 4566 0 33.8 0.092\n",
      "buy: 20190110-14-12 4569 1 16.3 0.102\n",
      "buy: 20190407-05-10 4572 0 33.7 0.105\n",
      "buy: 20190419-16-07 4574 0 22.6 0.095\n",
      "buy: 20190515-22-01 4578 0 18.3 0.142\n",
      "buy: 20190515-22-01 4578 1 15.9 0.099\n",
      "buy: 20190529-19-04 4583 0 32.2 0.09\n",
      "buy: 20190529-19-04 4583 4 63.0 0.082\n",
      "buy: 20190103-08-04 4585 0 41.1 0.106\n",
      "buy: 20190509-15-07 4586 0 64.3 0.123\n",
      "buy: 20190509-15-07 4586 1 53.2 0.08\n",
      "buy: 20190507-24-02 4594 0 18.3 0.083\n",
      "buy: 20190210-06-05 4597 0 21.4 0.081\n",
      "buy: 20190403-13-06 4605 0 107.4 0.099\n",
      "buy: 20190108-05-04 4622 0 39.0 0.109\n",
      "buy: 20190108-05-04 4622 5 41.0 0.081\n",
      "buy: 20190406-02-07 4624 0 16.7 0.093\n",
      "buy: 20190125-11-07 4626 0 16.6 0.109\n",
      "buy: 20190125-11-07 4626 5 63.7 0.093\n",
      "buy: 20190503-04-02 4632 0 45.0 0.109\n",
      "buy: 20190106-17-06 4637 0 22.5 0.083\n",
      "buy: 20190222-05-05 4647 0 36.0 0.088\n",
      "buy: 20190222-05-05 4647 1 20.1 0.089\n",
      "buy: 20190227-24-08 4651 0 23.6 0.081\n",
      "buy: 20190503-06-06 4652 0 99.2 0.1\n",
      "buy: 20190525-09-03 4653 0 51.7 0.111\n",
      "buy: 20190115-18-05 4664 0 20.8 0.086\n",
      "buy: 20190505-06-06 4666 0 42.5 0.135\n",
      "buy: 20190402-07-09 4668 0 18.9 0.081\n",
      "buy: 20190320-18-09 4684 0 18.4 0.087\n",
      "buy: 20190507-22-03 4687 0 16.8 0.126\n",
      "buy: 20190421-02-12 4696 0 35.7 0.13\n",
      "buy: 20190525-07-03 4697 0 22.7 0.144\n",
      "buy: 20190525-07-03 4697 1 13.0 0.127\n",
      "buy: 20190104-06-08 4698 0 51.0 0.102\n",
      "buy: 20190104-06-08 4698 1 70.9 0.085\n",
      "buy: 20190120-02-12 4705 0 86.1 0.095\n",
      "☆hit!☆: 20190120-02-12 0 86.1\n",
      "buy: 20190502-12-08 4707 0 24.3 0.191\n",
      "buy: 20190502-12-08 4707 4 29.9 0.084\n",
      "buy: 20190419-12-03 4709 1 37.7 0.113\n",
      "buy: 20190104-13-08 4712 0 44.3 0.134\n",
      "buy: 20190104-13-08 4712 1 14.2 0.107\n",
      "buy: 20190206-14-07 4713 0 9.9 0.156\n",
      "buy: 20190312-10-02 4731 5 38.8 0.084\n",
      "buy: 20190218-06-09 4741 0 13.1 0.118\n",
      "buy: 20190507-04-02 4744 0 53.0 0.094\n",
      "buy: 20190117-09-04 4746 0 19.2 0.099\n",
      "buy: 20190109-07-10 4749 0 18.0 0.112\n",
      "buy: 20190429-07-02 4753 0 15.5 0.115\n",
      "buy: 20190429-07-02 4753 1 17.4 0.087\n",
      "buy: 20190219-08-05 4754 0 26.8 0.082\n",
      "buy: 20190301-18-09 4757 0 22.5 0.129\n",
      "buy: 20190103-07-12 4767 0 25.2 0.082\n",
      "buy: 20190426-01-03 4772 0 77.4 0.081\n",
      "buy: 20190515-01-11 4780 0 18.1 0.091\n",
      "☆hit!☆: 20190515-01-11 0 18.1\n",
      "buy: 20190421-17-05 4783 1 21.3 0.093\n",
      "buy: 20190127-01-01 4789 0 65.2 0.133\n",
      "buy: 20190113-02-01 4792 0 22.6 0.145\n",
      "buy: 20190113-02-01 4792 1 27.4 0.102\n",
      "buy: 20190113-02-01 4792 6 30.4 0.082\n",
      "buy: 20190429-14-06 4809 0 29.6 0.096\n",
      "buy: 20190429-14-06 4809 1 104.2 0.083\n",
      "buy: 20190220-03-02 4810 0 20.5 0.081\n",
      "buy: 20190228-03-10 4820 0 21.3 0.085\n",
      "buy: 20190406-21-01 4831 0 38.0 0.085\n",
      "buy: 20190524-05-03 4834 0 23.9 0.1\n",
      "buy: 20190218-19-04 4845 0 52.3 0.088\n",
      "buy: 20190112-12-02 4849 0 44.8 0.135\n",
      "buy: 20190112-12-02 4849 1 51.1 0.107\n",
      "buy: 20190509-15-02 4850 0 38.9 0.097\n",
      "buy: 20190419-07-02 4867 0 17.9 0.096\n",
      "☆hit!☆: 20190419-07-02 0 17.9\n",
      "buy: 20190104-04-01 4872 0 138.8 0.156\n",
      "buy: 20190501-10-03 4873 0 17.8 0.094\n",
      "buy: 20190511-17-07 4874 0 51.9 0.086\n",
      "buy: 20190314-03-04 4876 0 112.0 0.081\n",
      "buy: 20190203-17-09 4877 0 13.7 0.11\n",
      "buy: 20190529-14-06 4888 0 32.7 0.083\n",
      "buy: 20190417-16-10 4896 0 15.5 0.175\n",
      "buy: 20190417-16-10 4896 2 55.6 0.083\n",
      "buy: 20190126-02-03 4904 0 18.0 0.084\n",
      "buy: 20190312-03-03 4905 0 30.6 0.121\n",
      "buy: 20190425-02-01 4906 1 23.1 0.081\n",
      "buy: 20190320-15-10 4926 0 15.2 0.106\n",
      "buy: 20190131-02-11 4928 0 29.4 0.084\n",
      "buy: 20190521-04-11 4929 0 36.5 0.095\n",
      "buy: 20190215-17-02 4931 1 28.5 0.092\n",
      "buy: 20190112-23-04 4935 0 18.5 0.102\n",
      "buy: 20190417-20-01 4945 0 18.0 0.095\n",
      "buy: 20190418-10-06 4948 0 25.6 0.083\n",
      "buy: 20190312-03-11 4950 0 29.7 0.087\n",
      "buy: 20190225-20-06 4956 0 86.3 0.084\n",
      "buy: 20190507-21-05 4957 0 16.2 0.099\n",
      "buy: 20190411-11-09 4966 0 39.2 0.1\n",
      "buy: 20190112-09-01 4968 0 21.2 0.115\n",
      "buy: 20190321-13-04 4975 0 34.8 0.101\n",
      "buy: 20190321-13-04 4975 1 30.4 0.103\n",
      "buy: 20190419-13-11 4977 0 18.6 0.166\n",
      "buy: 20190429-13-09 4979 0 34.6 0.084\n",
      "buy: 20190520-17-10 4989 0 10.6 0.143\n",
      "buy: 20190124-08-05 4993 0 18.8 0.084\n",
      "buy: 20190221-05-05 4996 0 44.3 0.141\n",
      "buy: 20190221-05-05 4996 1 44.0 0.092\n",
      "☆hit!☆: 20190221-05-05 1 44.0\n",
      "buy: 20190406-19-06 5002 0 21.6 0.106\n",
      "buy: 20190427-16-07 5003 0 27.0 0.107\n",
      "buy: 20190429-07-01 5005 0 13.3 0.196\n",
      "☆hit!☆: 20190429-07-01 0 13.3\n",
      "buy: 20190210-02-04 5012 0 57.0 0.117\n",
      "buy: 20190108-21-07 5018 1 20.6 0.083\n",
      "buy: 20190108-21-07 5018 5 38.2 0.083\n",
      "buy: 20190327-04-01 5020 0 13.9 0.112\n",
      "buy: 20190412-21-02 5037 0 44.5 0.106\n",
      "buy: 20190214-16-05 5042 0 24.5 0.1\n",
      "☆hit!☆: 20190214-16-05 0 24.5\n",
      "buy: 20190507-16-01 5044 0 21.1 0.087\n",
      "buy: 20190117-09-12 5047 0 10.4 0.197\n",
      "buy: 20190117-09-12 5047 1 16.0 0.136\n",
      "buy: 20190404-22-10 5050 0 22.2 0.101\n",
      "☆hit!☆: 20190404-22-10 0 22.2\n",
      "buy: 20190404-22-10 5050 1 23.7 0.085\n",
      "buy: 20190314-11-08 5057 0 35.1 0.086\n",
      "buy: 20190509-02-11 5062 0 20.5 0.097\n",
      "☆hit!☆: 20190509-02-11 0 20.5\n",
      "buy: 20190223-03-07 5065 0 28.7 0.093\n",
      "buy: 20190407-02-03 5067 0 64.3 0.087\n",
      "buy: 20190201-16-07 5073 0 22.7 0.101\n",
      "buy: 20190514-05-01 5076 0 58.0 0.081\n",
      "buy: 20190504-06-01 5091 0 46.9 0.094\n",
      "buy: 20190306-02-09 5098 0 24.1 0.121\n",
      "☆hit!☆: 20190306-02-09 0 24.1\n",
      "buy: 20190306-02-09 5098 1 25.7 0.081\n",
      "buy: 20190104-04-12 5099 0 20.8 0.11\n",
      "buy: 20190104-01-07 5101 0 66.6 0.118\n",
      "buy: 20190115-04-10 5102 0 30.5 0.084\n",
      "buy: 20190105-10-10 5103 0 28.3 0.15\n",
      "buy: 20190105-10-10 5103 1 14.1 0.153\n",
      "buy: 20190427-12-02 5105 0 18.8 0.082\n",
      "buy: 20190509-18-10 5107 0 23.5 0.089\n",
      "buy: 20190111-14-09 5114 0 26.1 0.152\n",
      "buy: 20190528-23-05 5115 0 20.2 0.126\n",
      "buy: 20190413-20-01 5129 0 13.6 0.13\n",
      "☆hit!☆: 20190413-20-01 0 13.6\n",
      "buy: 20190511-06-12 5130 0 20.3 0.096\n",
      "buy: 20190528-20-09 5131 0 60.8 0.098\n",
      "buy: 20190506-04-07 5133 0 68.1 0.121\n",
      "☆hit!☆: 20190506-04-07 0 68.1\n",
      "buy: 20190304-14-09 5134 0 18.4 0.167\n",
      "buy: 20190520-18-09 5140 0 22.0 0.101\n",
      "buy: 20190410-05-06 5141 1 126.6 0.085\n",
      "buy: 20190224-23-08 5155 0 53.8 0.194\n",
      "☆hit!☆: 20190224-23-08 0 53.8\n",
      "buy: 20190224-23-08 5155 1 29.1 0.115\n",
      "buy: 20190302-18-06 5159 0 90.3 0.102\n",
      "buy: 20190410-05-04 5161 0 36.4 0.104\n",
      "buy: 20190223-16-08 5164 0 20.4 0.134\n",
      "buy: 20190223-16-08 5164 1 50.0 0.093\n",
      "buy: 20190202-12-06 5169 0 41.0 0.083\n",
      "buy: 20190311-04-08 5172 0 46.6 0.093\n",
      "buy: 20190328-02-02 5174 0 125.7 0.082\n",
      "buy: 20190305-16-08 5182 0 21.8 0.17\n",
      "buy: 20190303-07-05 5185 0 31.0 0.108\n",
      "buy: 20190303-07-05 5185 1 55.0 0.082\n",
      "buy: 20190221-13-04 5196 0 28.7 0.095\n",
      "buy: 20190310-12-06 5207 0 18.0 0.088\n",
      "buy: 20190224-08-04 5215 0 21.4 0.132\n",
      "buy: 20190224-08-04 5215 4 22.3 0.082\n",
      "buy: 20190308-18-03 5216 0 24.0 0.153\n",
      "buy: 20190509-11-12 5222 0 19.2 0.133\n",
      "buy: 20190509-11-12 5222 1 21.8 0.088\n",
      "buy: 20190324-08-08 5226 0 21.5 0.151\n",
      "buy: 20190324-08-08 5226 1 29.6 0.086\n",
      "buy: 20190103-22-09 5229 0 15.6 0.151\n",
      "buy: 20190526-09-06 5241 0 28.6 0.115\n",
      "buy: 20190526-09-06 5241 5 41.9 0.086\n",
      "buy: 20190505-07-07 5242 0 33.0 0.094\n",
      "buy: 20190115-12-02 5245 0 42.6 0.084\n",
      "buy: 20190115-12-02 5245 1 26.2 0.086\n",
      "buy: 20190305-07-08 5246 0 16.0 0.111\n",
      "☆hit!☆: 20190305-07-08 0 16.0\n",
      "buy: 20190206-19-04 5249 0 12.0 0.127\n",
      "buy: 20190206-19-04 5249 5 46.6 0.084\n",
      "buy: 20190317-24-11 5261 1 15.2 0.101\n",
      "buy: 20190118-14-07 5262 0 32.0 0.081\n",
      "buy: 20190105-05-02 5267 0 19.8 0.083\n",
      "buy: 20190105-05-02 5267 1 47.0 0.081\n",
      "buy: 20190526-10-08 5270 0 21.5 0.086\n",
      "buy: 20190313-01-07 5281 0 39.7 0.085\n",
      "buy: 20190116-01-06 5287 0 15.4 0.099\n",
      "buy: 20190508-07-12 5288 0 14.8 0.111\n",
      "buy: 20190508-07-12 5288 1 24.8 0.094\n",
      "buy: 20190123-18-10 5289 0 21.4 0.083\n",
      "buy: 20190123-18-10 5289 1 76.7 0.084\n",
      "buy: 20190305-11-03 5295 0 47.4 0.116\n",
      "buy: 20190429-20-10 5296 0 11.8 0.134\n",
      "buy: 20190312-17-06 5297 0 37.0 0.104\n",
      "buy: 20190331-06-08 5299 0 12.2 0.183\n",
      "buy: 20190331-06-08 5299 1 19.9 0.103\n",
      "☆hit!☆: 20190331-06-08 1 19.9\n",
      "buy: 20190429-05-03 5301 0 166.3 0.086\n",
      "☆hit!☆: 20190429-05-03 0 166.3\n",
      "buy: 20190526-01-05 5310 0 24.5 0.149\n",
      "buy: 20190414-05-04 5317 0 33.9 0.084\n",
      "buy: 20190403-21-03 5318 0 35.5 0.105\n",
      "buy: 20190517-20-04 5322 0 94.2 0.129\n",
      "buy: 20190403-04-10 5324 1 26.5 0.086\n",
      "buy: 20190104-07-04 5331 0 28.9 0.175\n",
      "☆hit!☆: 20190104-07-04 0 28.9\n",
      "buy: 20190104-07-04 5331 1 43.9 0.095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buy: 20190123-17-06 5334 0 17.2 0.152\n",
      "buy: 20190428-14-10 5346 0 27.2 0.121\n",
      "buy: 20190427-18-05 5352 0 23.5 0.087\n",
      "buy: 20190109-14-12 5353 1 17.6 0.095\n",
      "buy: 20190511-01-02 5362 0 84.7 0.115\n",
      "buy: 20190511-01-02 5362 1 192.1 0.102\n",
      "buy: 20190523-22-04 5364 0 19.9 0.09\n",
      "buy: 20190322-03-05 5365 0 32.5 0.081\n",
      "buy: 20190121-04-06 5369 0 145.4 0.087\n",
      "buy: 20190412-14-04 5376 0 42.1 0.102\n",
      "buy: 20190307-24-07 5377 0 37.4 0.153\n",
      "buy: 20190307-24-07 5377 1 31.9 0.109\n",
      "buy: 20190521-17-09 5380 0 24.3 0.082\n",
      "buy: 20190330-18-07 5381 0 25.6 0.109\n",
      "buy: 20190315-04-02 5383 0 102.6 0.134\n",
      "buy: 20190315-04-02 5383 1 159.4 0.082\n",
      "buy: 20190118-03-05 5384 0 39.0 0.138\n",
      "buy: 20190118-03-05 5384 1 49.7 0.081\n",
      "buy: 20190425-02-04 5385 0 31.2 0.144\n",
      "buy: 20190204-03-06 5388 1 22.8 0.166\n",
      "buy: 20190204-03-06 5388 4 21.7 0.092\n",
      "buy: 20190204-03-06 5388 5 56.0 0.084\n",
      "buy: 20190102-03-11 5390 0 16.0 0.183\n",
      "buy: 20190416-19-08 5394 0 15.3 0.108\n",
      "buy: 20190416-19-08 5394 1 65.3 0.084\n",
      "buy: 20190330-14-02 5397 0 20.4 0.119\n",
      "buy: 20190526-14-09 5408 0 24.9 0.082\n",
      "buy: 20190516-02-05 5412 0 46.7 0.116\n",
      "☆hit!☆: 20190516-02-05 0 46.7\n",
      "buy: 20190219-13-06 5414 0 18.3 0.089\n",
      "buy: 20190415-22-09 5416 0 51.9 0.105\n",
      "buy: 20190415-22-09 5416 1 22.7 0.101\n",
      "buy: 20190224-05-04 5417 0 21.9 0.136\n",
      "buy: 20190224-05-04 5417 1 34.1 0.147\n",
      "buy: 20190428-18-10 5425 0 38.8 0.103\n",
      "buy: 20190314-08-01 5430 0 19.6 0.09\n",
      "buy: 20190527-08-05 5431 0 22.5 0.122\n",
      "buy: 20190318-14-07 5432 0 28.1 0.13\n",
      "buy: 20190407-08-07 5435 0 54.5 0.116\n",
      "buy: 20190111-03-08 5440 0 65.7 0.159\n",
      "buy: 20190111-03-08 5440 1 17.3 0.098\n",
      "buy: 20190527-08-09 5444 0 76.1 0.095\n",
      "buy: 20190519-06-05 5445 0 23.5 0.093\n",
      "buy: 20190404-07-04 5446 0 26.8 0.094\n",
      "buy: 20190501-13-06 5451 0 33.3 0.081\n",
      "buy: 20190224-15-09 5452 1 25.5 0.093\n",
      "buy: 20190227-22-09 5453 0 17.6 0.102\n",
      "buy: 20190428-06-07 5463 0 18.0 0.132\n",
      "buy: 20190203-05-08 5466 0 17.5 0.089\n",
      "buy: 20190322-05-02 5469 0 19.5 0.083\n",
      "buy: 20190122-19-05 5478 0 13.3 0.129\n",
      "buy: 20190122-19-05 5478 5 22.1 0.091\n",
      "buy: 20190418-11-08 5479 0 66.8 0.15\n",
      "buy: 20190418-11-08 5479 1 44.8 0.098\n",
      "buy: 20190327-16-07 5484 0 40.7 0.087\n",
      "buy: 20190125-24-03 5490 0 43.5 0.103\n",
      "☆hit!☆: 20190125-24-03 0 43.5\n",
      "buy: 20190120-02-01 5503 0 11.2 0.142\n",
      "buy: 20190128-06-07 5507 0 55.9 0.119\n",
      "buy: 20190115-08-06 5518 1 32.3 0.082\n",
      "buy: 20190120-04-11 5519 0 64.2 0.092\n",
      "buy: 20190105-21-05 5524 1 66.9 0.086\n",
      "buy: 20190402-10-02 5547 1 12.4 0.122\n",
      "buy: 20190104-24-10 5549 1 16.5 0.114\n",
      "buy: 20190210-16-08 5550 0 17.5 0.104\n",
      "buy: 20190518-03-07 5551 0 20.3 0.177\n",
      "buy: 20190518-03-07 5551 4 25.7 0.087\n",
      "buy: 20190302-03-11 5553 0 17.0 0.101\n",
      "buy: 20190322-06-08 5560 0 23.7 0.1\n",
      "buy: 20190208-19-05 5564 0 21.8 0.101\n",
      "buy: 20190219-01-01 5569 0 23.1 0.197\n",
      "buy: 20190219-01-01 5569 1 19.0 0.181\n",
      "buy: 20190219-01-01 5569 4 26.9 0.087\n",
      "buy: 20190222-08-03 5580 0 70.8 0.086\n",
      "buy: 20190223-21-09 5588 0 16.6 0.113\n",
      "buy: 20190423-02-12 5591 0 14.9 0.153\n",
      "buy: 20190423-02-12 5591 1 29.6 0.086\n",
      "buy: 20190430-15-12 5600 0 36.6 0.087\n",
      "buy: 20190110-09-06 5601 0 46.5 0.095\n",
      "buy: 20190112-04-02 5609 0 42.4 0.091\n",
      "buy: 20190204-10-05 5639 0 21.5 0.117\n",
      "buy: 20190326-03-07 5642 0 26.2 0.092\n",
      "☆hit!☆: 20190326-03-07 0 26.2\n",
      "buy: 20190210-06-02 5648 0 45.0 0.127\n",
      "buy: 20190210-06-02 5648 1 83.1 0.116\n",
      "buy: 20190131-08-03 5652 0 10.2 0.171\n",
      "buy: 20190316-22-06 5654 0 50.5 0.117\n",
      "buy: 20190316-22-06 5654 1 63.2 0.099\n",
      "buy: 20190413-14-08 5665 0 44.8 0.084\n",
      "☆hit!☆: 20190413-14-08 0 44.8\n",
      "buy: 20190106-05-11 5667 0 26.1 0.088\n",
      "buy: 20190225-15-03 5676 0 66.8 0.113\n",
      "buy: 20190208-04-11 5677 0 22.2 0.117\n",
      "buy: 20190116-20-01 5681 0 21.7 0.105\n",
      "buy: 20190114-06-07 5685 0 36.3 0.081\n",
      "buy: 20190524-15-06 5686 1 16.2 0.093\n",
      "buy: 20190209-02-02 5688 0 93.4 0.088\n",
      "buy: 20190209-24-08 5689 0 33.6 0.129\n",
      "☆hit!☆: 20190209-24-08 0 33.6\n",
      "buy: 20190206-11-06 5693 0 15.9 0.156\n",
      "☆hit!☆: 20190206-11-06 0 15.9\n",
      "buy: 20190206-11-06 5693 1 15.9 0.158\n",
      "buy: 20190220-21-02 5698 0 36.5 0.088\n",
      "buy: 20190105-01-03 5717 0 22.5 0.083\n",
      "buy: 20190412-09-10 5723 0 14.4 0.105\n"
     ]
    }
   ],
   "source": [
    "# testの回収率をシミュレート\n",
    "# オッズを見て判断する場合\n",
    "resAmount=0\n",
    "buyAmount=0\n",
    "resCnt=0\n",
    "buyCnt=0\n",
    "\n",
    "testPredictList=net.model.predict(X_test,batch_size=len(X_test))\n",
    "\n",
    "for i in range(len(raceId_test) ):\n",
    "    raceId=raceId_train[i]\n",
    "    with dbh.cursor() as cursor:\n",
    "        sel_sql = \"select funaken,odds from raceodds \\\n",
    "                   where oddsType = '3t' \\\n",
    "                   and raceId = '%s' \\\n",
    "                   order by funaken\" \\\n",
    "                   % (raceId)\n",
    "\n",
    "        cursor.execute(sel_sql)\n",
    "        loadList=pd.DataFrame(cursor.fetchall())\n",
    "        loadList=pd.DataFrame(loadList.replace(funakenDict))\n",
    "                \n",
    "    for j in range(120):\n",
    "        # y_predの閾値を下げてみる。\n",
    "        if testPredictList[i][j]> 0.08 and ( (testPredictList[i][j] * (loadList[loadList['funaken']==j]['odds'])).values[0] > 1.5) :\n",
    "            print(\"buy:\",raceId,i,j,loadList[loadList['funaken']==j]['odds'].values[0],round(testPredictList[i][j],3) )\n",
    "            buyAmount+=1\n",
    "            buyCnt+=1\n",
    "            if list(y_test[i]).index(1)==j:\n",
    "                print(\"☆hit!☆:\",raceId,j,loadList[loadList['funaken']==j]['odds'].values[0])\n",
    "                resAmount += o_test[i]\n",
    "                resCnt+=1\n",
    "            else:\n",
    "                pass\n",
    "#res=net.model.predict_on_batch(xdf[1:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resultReturn: 0.7087349397590362\n",
      "totalRace,buy,return 5726 1328 941.2000000000002\n"
     ]
    }
   ],
   "source": [
    "print(\"resultReturn:\",resAmount/buyAmount)\n",
    "print(\"totalRace,buy,return\",len(y_test),buyAmount,resAmount )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
